{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Inference: Selective attention vs. baseline\n",
    "This notebook handles the inference phase using **LLaVA-Next-Video**. \\\n",
    "It integrates the segmentation masks generated with SAM to perform \"Selective Attention\" called *Crop-and-Mask*, forcing the model to focus only on the objects of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up\n",
    "Import all necessary libraries for video processing, deep learning, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# --- Hardware & Memory Configuration ---\n",
    "# Prevents warnings from the tokenizer library\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Optimization to handle long videos and high VRAM usage\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Optional: Specify which GPU to use. \n",
    "# In a shared environment, '0' is usually the default.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import string\n",
    "import sys\n",
    "import traceback\n",
    "from typing import List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import av\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "# Download necessary NLTK data for evaluation metrics\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Hugging Face Libraries for LLaVaNextVideo\n",
    "from transformers import (\n",
    "    LlavaNextVideoProcessor, \n",
    "    LlavaNextVideoForConditionalGeneration, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# --- Logging and Warnings Setup ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration \n",
    "Configure the directory paths for original videos and SAM masks, sets model parameters, and identifies the file paths for Ground Truth data and result logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Directory Paths  ---\n",
    "original_video_dir = \"/path/to/your/videos/directory\"\n",
    "mask_dir_base = \"/path/to/your/output/masks\"\n",
    "GROUND_TRUTH_PATH = \"/path/to/your/ground_truth_answer.json\"\n",
    "PROMPTS_JSON_PATH = '/path/to/your/prompts.json'\n",
    "OUTPUT_DIR = \"/path/to/your/output\"\n",
    "\n",
    "# --- Model & Processing Parameters ---\n",
    "model_id = \"llava-hf/llava-next-video-7b-hf\"\n",
    "target_size = 224\n",
    "patch_size = 16\n",
    "max_frames = 24\n",
    "\n",
    "# --- Output Table Paths ---\n",
    "DETAILED_TABLE_PATH = os.path.join(OUTPUT_DIR, \"detailed_evaluation_table.csv\")\n",
    "SUMMARY_TABLE_PATH = os.path.join(OUTPUT_DIR, \"summary_accuracy_table.csv\")\n",
    "\n",
    "# Load ground truth answers\n",
    "try:\n",
    "    with open(GROUND_TRUTH_PATH, 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Ground truth file not found at {GROUND_TRUTH_PATH}\")\n",
    "    ground_truth = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utility Functions\n",
    "Helper functions for managing prompts, normalizing text responses, and categorizing questions into *Categorical* (Keyword matching) or *Open-Ended* (Semantic similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "#       Prompts Loader\n",
    "# =============================\n",
    "def load_prompts_from_file():\n",
    "    \"\"\"Retrieves prompt dictionary from JSON.\"\"\"\n",
    "    try:\n",
    "        with open(PROMPTS_JSON_PATH, 'r') as f:\n",
    "            prompts = json.load(f)\n",
    "        print(f\"Loaded {len(prompts)} prompts from {PROMPTS_JSON_PATH}\")\n",
    "        return prompts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompts.json: {e}\")\n",
    "        return {}\n",
    "\n",
    "# =============================\n",
    "#  Text & Evaluation Utilities\n",
    "# =============================\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalizes text for comparison.\"\"\"\n",
    "    return text.lower().translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "\n",
    "def split_questions(prompt_text):\n",
    "    \"\"\"Splits prompts into individual questions based on numbering.\"\"\"\n",
    "    questions = re.split(r'\\d+\\.\\s*', prompt_text)\n",
    "    questions = [q.strip() for q in questions if q.strip()]\n",
    "    if len(questions) > 1 and not any(word in questions[0].lower() for word in [\"?\", \"what\", \"how\", \"why\", \"list\"]):\n",
    "        questions = questions[1:]\n",
    "    return questions\n",
    "\n",
    "def categorize_question(question_text):\n",
    "    \"\"\"Categorizes questions as 'open-ended' or 'categorical' based on keyword matching.\"\"\"\n",
    "    question_text = question_text.lower().strip()\n",
    "    open_ended_keywords = [\n",
    "        \"why\", \"describe\", \"explain\", \"reason\", \"cause\", \"effect\", \"list\"\n",
    "    ]\n",
    "    question_text_clean = re.sub(r'[^\\w\\s]', '', question_text)\n",
    "    words = question_text_clean.split()\n",
    "    if words and words[0] in open_ended_keywords:\n",
    "        return \"open-ended\"\n",
    "    for keyword in open_ended_keywords:\n",
    "        if keyword in question_text_clean:\n",
    "            return \"open-ended\"\n",
    "    return \"categorical\"\n",
    "\n",
    "# =============================\n",
    "#    Visualization Utilities\n",
    "# =============================\n",
    "def save_attention_overlay(frame, attn_map, save_path, title=None, cmap='jet', alpha=0.5):\n",
    "    \"\"\"Overlays an attention/mask heatmap onto a video frame.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    if isinstance(frame, Image.Image):\n",
    "        frame_np = np.array(frame)\n",
    "    else:\n",
    "        frame_np = frame\n",
    "    plt.imshow(frame_np)\n",
    "    if attn_map is not None:\n",
    "        plt.imshow(attn_map, cmap=cmap, alpha=alpha)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_attention_for_mode(video_name, mode, frames, masks=None, output_base=\"attention_maps\"):\n",
    "    \"\"\"Saves a sequence of frames with their respective attention overlays.\"\"\"\n",
    "    mode_dir = os.path.join(output_base, video_name, mode)\n",
    "    os.makedirs(mode_dir, exist_ok=True)\n",
    "    logger.debug(f\"Visualizing attention for {video_name} in mode {mode}, {len(frames)} frames\")\n",
    "    for idx, frame in enumerate(frames):\n",
    "        try:\n",
    "            logger.debug(f\"Processing frame {idx}, frame type: {type(frame)}\")\n",
    "            if mode == \"original\":\n",
    "                attn_map = np.ones(frame.size[::-1]) if isinstance(frame, Image.Image) else np.ones(frame.shape[:2])\n",
    "            elif mode == \"crop-and-mask\":\n",
    "                mask = masks[idx] if masks and idx < len(masks) else None\n",
    "                attn_map = mask if mask is not None else np.zeros(frame.size[::-1] if isinstance(frame, Image.Image) else frame.shape[:2])\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported mode {mode} for visualization\")\n",
    "                continue\n",
    "            save_path = os.path.join(mode_dir, f\"frame_{idx:03d}.png\")\n",
    "            save_attention_overlay(frame, attn_map, save_path, title=f\"{mode} - frame {idx}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to visualize frame {idx} for {video_name} in mode {mode}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# =============================\n",
    "#     Evaluation Functions \n",
    "# =============================\n",
    "def evaluate_responses(video_id, model_answers, ground_truth, prompt, st_model):\n",
    "    \"\"\"\n",
    "    Computes Keyword Accuracy and Semantic Similarity.\n",
    "    Keyword Accuracy: Used for categorical (multiple choice) questions.\n",
    "    Semantic Similarity: Used for open-ended descriptive questions.\n",
    "    \"\"\"\n",
    "    if video_id not in ground_truth or not model_answers:\n",
    "        logger.warning(f\"No ground truth for video {video_id} or no model answers provided\")\n",
    "        return {}\n",
    "\n",
    "    gt_answers = ground_truth[video_id]\n",
    "    results = {}\n",
    "    for q_key in gt_answers:\n",
    "        question_text = prompt.get(q_key, \"\")\n",
    "        model_ans = model_answers.get(q_key, \"\")\n",
    "        gt_info = gt_answers[q_key]\n",
    "        if isinstance(gt_info, dict):\n",
    "            gt_ans = gt_info.get(\"answer\", \"\")\n",
    "            keywords = gt_info.get(\"keywords\", [])\n",
    "        else:\n",
    "            gt_ans = gt_info\n",
    "            keywords = []\n",
    "        category = categorize_question(question_text)\n",
    "\n",
    "        metrics = {\"category\": category}\n",
    "\n",
    "        if category == \"categorical\":\n",
    "            model_ans_lower = model_ans.lower()\n",
    "            pred_label = None\n",
    "            for kw in keywords:\n",
    "                if kw.lower() in model_ans_lower:\n",
    "                    pred_label = kw.lower()\n",
    "                    break\n",
    "            # Compute semantic similarity for categorical as well\n",
    "            try:\n",
    "                emb_model = st_model.encode([model_ans.strip(), gt_ans.strip()])\n",
    "                sim_score = util.cos_sim(emb_model[0], emb_model[1]).item()\n",
    "            except Exception:\n",
    "                sim_score = 0.0\n",
    "            # Accept if keyword match OR high semantic similarity\n",
    "            metrics[\"keyword_accuracy\"] = int((pred_label is not None) or (sim_score > 0.8))\n",
    "            metrics[\"semantic_similarity\"] = sim_score\n",
    "            metrics[\"predicted_label\"] = pred_label\n",
    "            metrics[\"ground_truth_label\"] = gt_ans.lower().strip()\n",
    "        else:\n",
    "            # Only semantic similarity for open-ended\n",
    "            try:\n",
    "                emb_model = st_model.encode([model_ans.strip(), gt_ans.strip()])\n",
    "                metrics[\"semantic_similarity\"] = util.cos_sim(emb_model[0], emb_model[1]).item()\n",
    "            except Exception:\n",
    "                metrics[\"semantic_similarity\"] = 0.0\n",
    "            metrics[\"keyword_accuracy\"] = None\n",
    "            metrics[\"predicted_label\"] = None\n",
    "            metrics[\"ground_truth_label\"] = None\n",
    "            \n",
    "        results[q_key] = {\n",
    "            \"question\": question_text,\n",
    "            \"ground_truth\": gt_ans,\n",
    "            \"model_answer\": model_ans,\n",
    "            **metrics\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vision-Tower Self-Attention Analysis\n",
    "This section contains the logic for probing the internal attention weights of the LLaVA vision encoder.\\\n",
    "These functions extract the *self-attention maps* generated by the model during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Vision-only Self-Attention Extraction\n",
    "# ======================================\n",
    "def extract_framewise_attention(model, frames):\n",
    "    \"\"\"\n",
    "    Extracts attention weights for each frame (image) individually.\n",
    "    Returns a list of attention matrices, one per frame.\n",
    "    \"\"\"\n",
    "    vision_tower = model.vision_tower\n",
    "    vision_model = vision_tower.vision_model\n",
    "    device = next(vision_model.parameters()).device\n",
    "\n",
    "    attn_per_frame = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Resize frame to 336x336 as required by the LLaVA vision tower\n",
    "        if isinstance(frame, Image.Image):\n",
    "            frame_resized = frame.resize((336, 336), Image.BICUBIC)\n",
    "            frame_np = np.array(frame_resized)\n",
    "        else:\n",
    "            frame_resized = cv2.resize(frame, (336, 336), interpolation=cv2.INTER_CUBIC)\n",
    "            frame_np = frame_resized\n",
    "        if frame_np.shape[-1] == 3:\n",
    "            frame_np = frame_np.transpose(2, 0, 1) # (3, H, W)\n",
    "        frame_tensor = torch.from_numpy(frame_np).unsqueeze(0).float().to(device) # (1, 3, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = vision_model(\n",
    "                pixel_values=frame_tensor,\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "        if outputs.attentions is not None and len(outputs.attentions) > 0:\n",
    "            attn_layers = []\n",
    "            for attn in outputs.attentions:\n",
    "                attn_mean = attn.mean(dim=(0,1)) # Mean across heads: (tokens, tokens)\n",
    "                attn_layers.append(attn_mean.cpu())\n",
    "            attn_per_frame.append(torch.stack(attn_layers)) # (layers, tokens, tokens)\n",
    "        else:\n",
    "            attn_per_frame.append(None)\n",
    "    return attn_per_frame\n",
    "\n",
    "# ======================================\n",
    "#       Statistical Visualization\n",
    "# ======================================\n",
    "def plot_layer_vs_frame_attention(attn_per_frame, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap with y-axis as layers and x-axis as frames.\n",
    "    Each cell shows the mean attention (over tokens) for that layer and frame.\n",
    "    \"\"\"\n",
    "    layer_frame_matrix = []\n",
    "    for i, attn_stack in enumerate(attn_per_frame):\n",
    "        if attn_stack is not None:\n",
    "            # Mean attention over all tokens (excluding CLS)\n",
    "            mean_per_layer = attn_stack[:, 0, 1:].mean(dim=1).cpu().numpy() # shape: (layers,)\n",
    "            layer_frame_matrix.append(mean_per_layer)\n",
    "        else:\n",
    "            layer_frame_matrix.append(np.full(attn_per_frame[0].shape[0], np.nan))\n",
    "\n",
    "    layer_frame_matrix = np.stack(layer_frame_matrix, axis=0).T  # (layers, frames)\n",
    "\n",
    "    # Normalize each layer (row) separately for better contrast\n",
    "    for i in range(layer_frame_matrix.shape[0]):\n",
    "        row = layer_frame_matrix[i]\n",
    "        min_val = np.nanmin(row)\n",
    "        max_val = np.nanmax(row)\n",
    "        if max_val > min_val:\n",
    "            layer_frame_matrix[i] = (row - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            layer_frame_matrix[i] = 0\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(layer_frame_matrix, cmap='viridis', xticklabels=True, yticklabels=True)\n",
    "    plt.xlabel(\"Frame Index\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Mean Attention per Layer vs Frame (Normalized)\")\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_mean_attention_over_frames(attn_per_frame, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the mean attention (averaged over all layers and tokens) for each frame.\n",
    "    \"\"\"\n",
    "    mean_per_frame = []\n",
    "    for attn_stack in attn_per_frame:\n",
    "        if attn_stack is not None:\n",
    "            mean_val = attn_stack[:, 0, 1:].mean().item()\n",
    "            mean_per_frame.append(mean_val)\n",
    "        else:\n",
    "            mean_per_frame.append(float('nan'))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(mean_per_frame, marker='o')\n",
    "    plt.xlabel(\"Frame Index\")\n",
    "    plt.ylabel(\"Mean Attention (all layers)\")\n",
    "    plt.title(\"Mean Attention per Frame (Averaged over Layers)\")\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ======================================\n",
    "#           Spatial Overlays\n",
    "# ======================================\n",
    "def plot_attention_overlay_on_frame(frame, attn_stack, save_path=None, blur=True):\n",
    "    \"\"\"Overlays internal mean attention map on input frame with Gaussian blur smoothing.\"\"\"\n",
    "    attn_matrix = attn_stack[-1, :, 1:]  # (tokens, patches)\n",
    "    attn_map = attn_matrix.mean(axis=0)  # Average over all source tokens\n",
    "    num_patches = attn_map.shape[0]\n",
    "    grid_size = int(np.sqrt(num_patches))\n",
    "    attn_map = attn_map.reshape(grid_size, grid_size)\n",
    "    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min() + 1e-8)\n",
    "\n",
    "    if isinstance(attn_map, torch.Tensor):\n",
    "        attn_map = attn_map.cpu().numpy()\n",
    "\n",
    "    if isinstance(frame, Image.Image):\n",
    "        frame_np = np.array(frame)\n",
    "    elif isinstance(frame, np.ndarray):\n",
    "        frame_np = frame\n",
    "    else:\n",
    "        raise ValueError(\"Frame must be a PIL Image or numpy array.\")\n",
    "\n",
    "    # Resize attention map to frame size\n",
    "    attn_map_resized = cv2.resize(attn_map.astype(np.float32), (frame_np.shape[1], frame_np.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Apply Gaussian blur for smoothness\n",
    "    if blur:\n",
    "        attn_map_resized = cv2.GaussianBlur(attn_map_resized, (0, 0), sigmaX=8, sigmaY=8)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(frame_np)\n",
    "    plt.imshow(attn_map_resized, cmap='jet', alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_attention_overlays_for_frames(model, frames, video_name, mode):\n",
    "    \"\"\"Saves attention heatmap overlays to the frame results directory.\"\"\"\n",
    "    base_dir = \"/path/to/your/attention_layer_LLAva\"\n",
    "    video_dir = os.path.join(base_dir, f\"{video_name}\")\n",
    "    frames_dir = os.path.join(video_dir, \"frames\", mode)\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    attn_per_frame = extract_framewise_attention(model, frames)\n",
    "    for idx, (frame, attn_stack) in enumerate(zip(frames, attn_per_frame)):\n",
    "        if attn_stack is not None:\n",
    "            save_path = os.path.join(frames_dir, f\"frame_{idx:03d}.png\")\n",
    "            plot_attention_overlay_on_frame(frame, attn_stack, save_path=save_path)\n",
    "\n",
    "def plot_attention_matrix(attn_stack):\n",
    "    \"\"\"Heatmap showing mean attention to tokens per layer.\"\"\"\n",
    "    attn_mean = attn_stack.mean(1).numpy()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(attn_mean, cmap='viridis', xticklabels=True, yticklabels=True)\n",
    "    plt.xlabel(\"Token Index\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.title(\"Temporal Attention: Layers vs Tokens\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"temporal_attention.png\")\n",
    "    plt.show() \n",
    "    plt.close()\n",
    "\n",
    "# ======================================\n",
    "#            Main Analysis\n",
    "# ======================================\n",
    "def analyze_attention_per_mode(model, processor, frames, video_name, mode=\"original\"):\n",
    "    \"\"\"\n",
    "    Extract and save the layer-vs-frame attention matrix AND mean attention per frame plot for a mode.\n",
    "    \"\"\"\n",
    "    base_dir = \"/path/to/your/attention_layer_LLAva\"\n",
    "    video_dir = os.path.join(base_dir, f\"{video_name}\")\n",
    "    mode_dir = os.path.join(video_dir, mode)\n",
    "    os.makedirs(mode_dir, exist_ok=True)\n",
    "\n",
    "    attn_per_frame = extract_framewise_attention(model, frames)\n",
    "    layer_frame_path = os.path.join(mode_dir, \"layer_vs_frame_attention.png\")\n",
    "    plot_layer_vs_frame_attention(attn_per_frame, save_path=layer_frame_path)\n",
    "    mean_frame_path = os.path.join(mode_dir, \"mean_attention_per_frame.png\")\n",
    "    plot_mean_attention_over_frames(attn_per_frame, save_path=mean_frame_path)\n",
    "\n",
    "    # Pad frames for crop-and-mask mode consistency\n",
    "    if mode == \"crop-and-mask\":\n",
    "        np_frames = [np.array(f) if hasattr(f, \"size\") else f for f in frames]\n",
    "        max_h = max(f.shape[0] for f in np_frames)\n",
    "        max_w = max(f.shape[1] for f in np_frames)\n",
    "        padded_frames = []\n",
    "        for f in np_frames:\n",
    "            h, w = f.shape[0], f.shape[1]\n",
    "            pad_h = max_h - h\n",
    "            pad_w = max_w - w\n",
    "            padded = np.pad(f, ((0, pad_h), (0, pad_w), (0,0)), mode='constant', constant_values=0)\n",
    "            padded_frames.append(padded)\n",
    "        frames = padded_frames\n",
    "\n",
    "    # Extract and plot\n",
    "    attn_per_frame = extract_framewise_attention(model, frames)\n",
    "\n",
    "    layer_frame_path = os.path.join(mode_dir, \"layer_vs_frame_attention.png\")\n",
    "    plot_layer_vs_frame_attention(attn_per_frame, save_path=layer_frame_path)\n",
    "    print(f\"[INFO] Saved layer-vs-frame attention heatmap to {layer_frame_path}\")\n",
    "\n",
    "    mean_frame_path = os.path.join(mode_dir, \"mean_attention_per_frame.png\")\n",
    "    plot_mean_attention_over_frames(attn_per_frame, save_path=mean_frame_path)\n",
    "    print(f\"[INFO] Saved mean attention per frame plot to {mean_frame_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Loading\n",
    "This class initializes LLaVA-Next-Video-7b model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"\n",
    "    Singleton class to ensure model is only loaded once into memory.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance.load_models()\n",
    "        return cls._instance\n",
    "    \n",
    "    def load_models(self):\n",
    "        # Clear existing memory to prevent OOM (Out of Memory) errors\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        try:\n",
    "            # Configure 4-bit quantization for efficient VRAM usage\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "\n",
    "            logger.info(\"Loading LLaVA-Next-Video model...\")\n",
    "            # Initialize Processor\n",
    "            self.processor = LlavaNextVideoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "            \n",
    "            # Initialize the Large Multimodal Model (LMM)\n",
    "            self.model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=quant_config,\n",
    "                device_map={\"\": \"cuda:0\"},\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            self.model.eval()\n",
    "\n",
    "            logger.info(\"Loading SentenceTransformer model...\")\n",
    "            # Load the embedding model on CPU to save GPU space for the main model\n",
    "            self.st_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "            \n",
    "            logger.info(\"All models loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model loading failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ“ Research Note: Discarded Experimental Mode\n",
    "During the development, an additional mode called **\"Full-Frame Focus\"** was implemented. \n",
    "\n",
    "**Methodology:**\n",
    "This mode utilized PyTorch forward hooks to intercept the Vision Transformer's hidden states. It applied a $2.0\\times$ scaling factor to tokens corresponding to masked regions, attempting to \"force\" focus while maintaining the original background context.\n",
    "\n",
    "**Results & Decision:**\n",
    "This mode was ultimately **discarded** in favor of the **Crop-and-Mask** approach. \n",
    "1. **Signal-to-Noise:** Physically masking the background (Crop-and-Mask) provided a significantly cleaner signal for the LMM.\n",
    "2. **Efficiency:** The hook-based approach introduced unnecessary complexity and potential numerical instability in 4-bit quantized layers.\n",
    "3. **Consistency:** Crop-and-Mask aligned better with standard computer vision preprocessing benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Video analysis\n",
    "The `VideoAnalyzer` class represent the data pipeline. It is responsible for sampling temporal data, aligning segmentation masks with visual frames, and executing the inference loop.\n",
    "\n",
    "1.  **Temporal Sampling:** Selecting key frames (start, middle, end) and intermediate steps to capture the surgical workflow within the 24-frame limit.\n",
    "2.  **Motion Analysis:** Computing pixel-level differences to understand activity intensity (for debugging and data filtering).\n",
    "3.  **Mask Fusion:** Combining multiple segmentations into a single unified mask per frame.\n",
    "4.  **Spatial Preprocessing:** In *Crop-and-Mask* mode, this class identifies the *Bounding Box* of the unified mask and crops the high-resolution frame to focus on the action.\n",
    "5.  **Clean Response Parsing:** A regex-based post-processor that extracts structured answers from the model's natural language output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAnalyzer:\n",
    "    def __init__(self):\n",
    "        models = ModelLoader()\n",
    "        self.processor = models.processor\n",
    "        self.model = models.model\n",
    "        self.st_model = models.st_model\n",
    "        self.results = []\n",
    "        self.frame_idx = 0 # Track current frame for debug logging\n",
    "    \n",
    "    def get_frame_indices(self, frame_count: int, max_frames: int = 16) -> List[int]:\n",
    "        \"\"\"Samples frames evenly, ensuring start, middle, and end are included.\"\"\"\n",
    "        if frame_count <= max_frames:\n",
    "            return list(range(frame_count))\n",
    "        step = max(1, frame_count // (max_frames - 3))\n",
    "        indices = list(range(0, frame_count, step))\n",
    "        key_frames = {0, frame_count // 2, frame_count - 1}\n",
    "        indices = sorted(list(set(indices + list(key_frames))))\n",
    "        indices = indices[:max_frames]\n",
    "        logger.debug(f\"Selected frame indices: {indices}\")\n",
    "        return indices\n",
    "    \n",
    "    def compute_motion_intensity(self, prev_frame: np.ndarray, curr_frame: np.ndarray) -> float:\n",
    "        \"\"\"Calculates binary temporal difference between frames to estimate activity.\"\"\"\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)\n",
    "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)\n",
    "        diff = cv2.absdiff(prev_gray, curr_gray)\n",
    "        _, diff = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY)\n",
    "        score = np.mean(diff)\n",
    "        logger.debug(f\"Motion intensity for frame: mean diff = {score}\")\n",
    "        return score\n",
    "    \n",
    "    def read_video_imageio(self, video_path: str, max_frames: int) -> tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"Standard ffmpeg reader via imageio.\"\"\"\n",
    "        try:\n",
    "            reader = imageio.get_reader(video_path, 'ffmpeg')\n",
    "            total_frames = reader.count_frames()\n",
    "            indices = self.get_frame_indices(total_frames, max_frames)\n",
    "            frames = [reader.get_data(idx) for idx in indices]\n",
    "            reader.close()\n",
    "            logger.info(f\"Imageio extracted {len(frames)} frames from {video_path}\")\n",
    "            return np.stack(frames), indices\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Imageio failed for {video_path}: {str(e)}\")\n",
    "            return np.array([]), []\n",
    "    \n",
    "        def read_video_pyav(self, video_path: str, max_frames: int = 16) -> tuple[List[np.ndarray], List[int]]:\n",
    "        \"\"\"Alternative reader using PyAV for better handling of variable frame rates.\"\"\"\n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            stream = container.streams.video[0]\n",
    "            total_frames = stream.frames if stream.frames > 0 else None\n",
    "            if total_frames is None:\n",
    "                logger.warning(f\"Video {video_path} reports 0 frames, attempting to count manually\")\n",
    "                total_frames = sum(1 for _ in container.decode(video=0))\n",
    "            indices = np.linspace(0, total_frames - 1, min(max_frames, total_frames), dtype=int)\n",
    "            frames = []\n",
    "            frame_indices = []\n",
    "            container.seek(0)\n",
    "            frame_count = 0\n",
    "            for frame in container.decode(video=0):\n",
    "                if frame_count in indices:\n",
    "                    frame_np = frame.to_ndarray(format='rgb24')\n",
    "                    frames.append(frame_np)\n",
    "                    frame_indices.append(frame_count)\n",
    "                frame_count += 1\n",
    "                if len(frames) >= max_frames:\n",
    "                    break\n",
    "            container.close()\n",
    "            logger.info(f\"Extracted {len(frames)} frames from {video_path}\")\n",
    "            return frames, frame_indices\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read video {video_path}: {str(e)}\")\n",
    "            return [], []\n",
    "    \n",
    "    def load_frame_masks(self, mask_dir: str, frame_indices: List[int], frame_size: tuple, video_name: str) -> dict:\n",
    "        \"\"\"Loads and combines .npy masks based on metadata.json for the specific sampled frames.\"\"\"\n",
    "        masks = {}\n",
    "        json_path = os.path.join(mask_dir, \"mask_metadata.json\")\n",
    "        if not os.path.exists(json_path):\n",
    "            logger.error(f\"No mask_metadata.json found in {mask_dir}\")\n",
    "            return {idx: None for idx in frame_indices}\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            available_frames = [int(k) for k in metadata.keys() if metadata[k]]\n",
    "            if not available_frames:\n",
    "                logger.warning(f\"No valid masks found in metadata\")\n",
    "                return {idx: None for idx in frame_indices}\n",
    "            first_mask_frame = min(available_frames)\n",
    "            last_mask_frame = max(available_frames)\n",
    "            logger.info(f\"Masks available from frame {first_mask_frame} to {last_mask_frame}\")\n",
    "            for idx in frame_indices:\n",
    "                if idx < first_mask_frame:\n",
    "                    masks[idx] = None\n",
    "                    logger.debug(f\"Frame {idx} is before first mask frame, setting to None\")\n",
    "                else:\n",
    "                    frame_key = str(idx)\n",
    "                    if frame_key in metadata and metadata[frame_key]:\n",
    "                        try:\n",
    "                            combined_mask = None\n",
    "                            for track in metadata[frame_key]:\n",
    "                                mask_file = os.path.join(mask_dir, track[\"mask_file\"])\n",
    "                                logger.debug(f\"Checking mask file: {mask_file}\")\n",
    "                                if os.path.exists(mask_file):\n",
    "                                    mask = np.load(mask_file)\n",
    "                                    logger.debug(f\"Loaded mask shape: {mask.shape}, dtype: {mask.dtype}, sum: {np.sum(mask)}\")\n",
    "                                    if mask.ndim > 2:\n",
    "                                        mask = mask.squeeze()\n",
    "                                    if mask.shape != frame_size[::-1]:\n",
    "                                        logger.debug(f\"Resizing mask from {mask.shape} to {frame_size[::-1]}\")\n",
    "                                        mask = cv2.resize(mask, frame_size, interpolation=cv2.INTER_NEAREST)\n",
    "                                    mask = (mask > 0).astype(np.uint8)\n",
    "                                    logger.debug(f\"Mask sum after binarization: {np.sum(mask)}\")\n",
    "                                    if np.sum(mask) == 0:\n",
    "                                        logger.warning(f\"Mask {mask_file} is empty after binarization\")\n",
    "                                        continue\n",
    "                                    if combined_mask is None:\n",
    "                                        combined_mask = mask\n",
    "                                    else:\n",
    "                                        combined_mask = np.logical_or(combined_mask, mask).astype(np.uint8)\n",
    "                            masks[idx] = combined_mask\n",
    "                            if combined_mask is not None:\n",
    "                                debug_dir = f\"debug_frames_{video_name}\"\n",
    "                                os.makedirs(debug_dir, exist_ok=True)\n",
    "                                cv2.imwrite(os.path.join(debug_dir, f\"combined_mask_{idx}.png\"), combined_mask * 255)\n",
    "                                logger.debug(f\"Saved combined mask for frame {idx}, shape: {combined_mask.shape}, sum: {np.sum(combined_mask)}\")\n",
    "                            else:\n",
    "                                logger.debug(f\"No valid combined mask for frame {idx}\")\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Error loading mask for frame {idx}: {str(e)}\")\n",
    "                            masks[idx] = None\n",
    "                    else:\n",
    "                        masks[idx] = None\n",
    "                        logger.debug(f\"No metadata or tracks for frame {idx}\")\n",
    "\n",
    "            mask_counts = sum(1 for m in masks.values() if m is not None)\n",
    "            logger.info(f\"Loaded {mask_counts}/{len(frame_indices)} masks successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading mask metadata: {str(e)}\")\n",
    "            return {idx: None for idx in frame_indices}\n",
    "        return masks\n",
    "        \n",
    "    def preprocess_frame(self, frame: Image.Image, mask: Optional[np.ndarray], mode: str) -> np.ndarray:\n",
    "        \"\"\"Handles cropping the frame to the bounding box.\"\"\"\n",
    "        frame = np.array(frame)\n",
    "        logger.debug(f\"Frame converted to np.ndarray, shape: {frame.shape}\")\n",
    "        if mode == \"crop-and-mask\" and mask is not None:\n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "            coords = cv2.findNonZero(mask)\n",
    "            if coords is not None:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                padding = 10\n",
    "                x1, y1 = max(x - padding, 0), max(y - padding, 0)\n",
    "                x2, y2 = min(x + w + padding, frame.shape[1]), min(y + h + padding, frame.shape[0])\n",
    "                cropped = frame[y1:y2, x1:x2]\n",
    "                logger.debug(f\"Cropped frame to ({y1}:{y2}, {x1}:{x2}), shape: {cropped.shape}\")\n",
    "                cv2.imwrite(f\"debug_frames_3/cropped_mask_{mode}_{self.frame_idx}.png\", cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR))\n",
    "                return cropped\n",
    "            else:\n",
    "                logger.warning(f\"No non-zero mask pixels for frame {self.frame_idx}, returning original frame\")\n",
    "        return frame\n",
    "            \n",
    "    def clean_response(self, response: str) -> dict:\n",
    "       \"\"\"Regex-based parser to strip system tags and convert the model output into a Q&A dictionary.\"\"\"\n",
    "        response = re.sub(r'<image>|</image>|<video>|</video>|\\[object.*?\\]|USER:.*?ASSISTANT:|\\bDescribe\\b.*?\\?|What does the hand do.*?\\?', '', response, flags=re.DOTALL)\n",
    "        response = re.sub(r'\\s+', ' ', response).strip()\n",
    "        if not response:\n",
    "            logger.warning(\"No response content after cleaning\")\n",
    "            return {\"error\": \"No valid answers parsed\"}\n",
    "        \n",
    "        logger.debug(f\"Raw response after cleaning: {response}\")\n",
    "        \n",
    "        answers = {}\n",
    "        pattern = r'(\\d+)\\.\\s*([^1-5]\\S.*?)(?=\\s*\\d+\\.\\s*|$)' \n",
    "        matches = re.finditer(pattern, response, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            q_num = match.group(1)\n",
    "            q_answer = match.group(2).strip()\n",
    "            if q_answer:\n",
    "                answers[f\"q{q_num}\"] = q_answer\n",
    "                logger.debug(f\"Parsed answer for q{q_num}: {q_answer}\")\n",
    "        \n",
    "        if not answers:\n",
    "            logger.debug(\"Regex parsing failed, attempting fallback parsing\")\n",
    "            lines = response.split(\"\\n\")\n",
    "            current_q = None\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                match = re.match(r'^(\\d+)\\.\\s*(.+)', line)\n",
    "                if match:\n",
    "                    current_q = f\"q{match.group(1)}\"\n",
    "                    answers[current_q] = match.group(2).strip()\n",
    "                    logger.debug(f\"Fallback: Parsed answer for {current_q}: {answers[current_q]}\")\n",
    "                elif current_q and line:\n",
    "                    answers[current_q] += \" \" + line.strip()\n",
    "                    logger.debug(f\"Fallback: Appended to {current_q}: {answers[current_q]}\")\n",
    "        \n",
    "        if not answers:\n",
    "            logger.warning(f\"Failed to parse response into answers: {response[:100]}...\")\n",
    "            return {\"error\": \"No valid answers parsed\"}\n",
    "        \n",
    "        logger.debug(f\"Final parsed answers: {answers}\")\n",
    "        return answers\n",
    "        \n",
    "    def generate_description(self, frames: List[Image.Image], masks: List, video_name: str, prompt: str, mode: str):\n",
    "        \"\"\"Prepares video tensors, runs inference, and extracts attention maps.\"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "        start_time = time.time()\n",
    "        conversation = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"video\"}\n",
    "            ]\n",
    "        }]\n",
    "        structured_prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Preparing inputs for {video_name} in mode {mode}\")\n",
    "            video_frames = [np.array(frame) for frame in frames]\n",
    "            if mode == \"crop-and-mask\":\n",
    "                # Ensure all crops are padded to the same size for batching\n",
    "                max_h = max(f.shape[0] for f in video_frames)\n",
    "                max_w = max(f.shape[1] for f in video_frames)\n",
    "                padded_frames = []\n",
    "                for f in video_frames:\n",
    "                    h, w = f.shape[0], f.shape[1]\n",
    "                    pad_h = max_h - h\n",
    "                    pad_w = max_w - w\n",
    "                    padded = np.pad(f, ((0, pad_h), (0, pad_w), (0,0)), mode='constant', constant_values=0)\n",
    "                    padded_frames.append(padded)\n",
    "                video_frames = padded_frames\n",
    "                logger.info(f\"Padded all crop-and-mask frames to shape ({max_h}, {max_w}, 3)\")\n",
    "            \n",
    "            for i, frame in enumerate(video_frames):\n",
    "                logger.debug(f\"Frame {i} shape: {frame.shape}\")\n",
    "            video_tensor = np.stack(video_frames)\n",
    "            video_tensor = video_tensor.transpose(0, 3, 1, 2)\n",
    "            video_tensor = np.expand_dims(video_tensor, 0)\n",
    "            logger.info(f\"Video tensor shape before processing: {video_tensor.shape}\")\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=structured_prompt,\n",
    "                videos=video_tensor,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                do_rescale=False\n",
    "            )\n",
    "            logger.info(f\"Processor output keys: {list(inputs.keys())}\")\n",
    "            inputs = {k: v.to(\"cuda:0\") for k, v in inputs.items()}\n",
    "            if 'pixel_values' in inputs:\n",
    "                inputs['pixel_values_videos'] = inputs['pixel_values_videos'].to(dtype=torch.float32)\n",
    "            logger.debug(f\"Input tensor shapes and dtypes: {[(k, v.shape, v.dtype) for k, v in inputs.items()]}\")\n",
    "                        \n",
    "            logger.info(f\"Running model inference for {video_name} in mode {mode}\")\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=800\n",
    "                )\n",
    "\n",
    "            # --- Extract and plot temporal attention ---\n",
    "            try:\n",
    "                attn_per_frame = extract_framewise_attention(self.model, frames)\n",
    "                matrix_path = f\"layer_vs_frame_attention_{video_name}_{mode}.png\"\n",
    "                plot_layer_vs_frame_attention(attn_per_frame, save_path=matrix_path)\n",
    "                logger.info(f\"Saved layer-vs-frame attention matrix for {video_name} [{mode}] to {matrix_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not extract or plot layer-vs-frame attention: {e}\")\n",
    "\n",
    "            plot_mean_attention_over_frames(attn_per_frame, save_path=\"mean_attention_per_frame.png\")\n",
    "            \n",
    "            response = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            answers = self.clean_response(response)\n",
    "            \n",
    "            logger.info(f\"Generated response for {video_name}: {response[:100]}...\")\n",
    "            return {\n",
    "                \"answers\": answers,\n",
    "                \"video_name\": video_name,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed for {video_name} in mode {mode}: {str(e)}\")\n",
    "            raise\n",
    "     \n",
    "    def process_video(self, video_path: str, mask_dir: Optional[str], mode: str) -> Tuple[List[Image.Image], List[Optional[np.ndarray]], float, int]:\n",
    "        \"\"\"Decodes video, samples frames, and applies preprocessing.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Attempting to open video: {video_path}\")\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "            # Manual count fallback if header is missing frame count\n",
    "            if frame_count <= 0:\n",
    "                logger.warning(f\"Video {video_path} reports 0 frames, attempting to count manually\")\n",
    "                frame_count = 0\n",
    "                while cap.isOpened():\n",
    "                    ret, _ = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame_count += 1\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            logger.info(f\"Extracted {frame_count} frames from {video_path}\")\n",
    "\n",
    "            frames = []\n",
    "            masks = []\n",
    "            self.frame_idx = 0\n",
    "            mask_files = []\n",
    "\n",
    "            # Locate mask files\n",
    "            if mask_dir and os.path.isdir(mask_dir):\n",
    "                mask_files = sorted(glob.glob(os.path.join(mask_dir, \"*.npy\")))\n",
    "                if mask_files:\n",
    "                    logger.info(f\"Masks available from frame 0 to {len(mask_files)-1}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No .npy mask files found in {mask_dir}. {mode} mode will use original frames.\")\n",
    "            else:\n",
    "                logger.warning(f\"Mask directory {mask_dir} does not exist. {mode} mode will use original frames.\")\n",
    "\n",
    "            # Sample frames evenly across the video\n",
    "            max_frames = 24\n",
    "            frame_indices = self.get_frame_indices(frame_count, max_frames)\n",
    "\n",
    "            for idx in frame_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_pil = Image.fromarray(frame)\n",
    "                mask = None\n",
    "\n",
    "                # Align mask with frame index\n",
    "                if mask_files and idx < len(mask_files):\n",
    "                    mask_path = mask_files[idx]\n",
    "                    try:\n",
    "                        mask = np.load(mask_path)\n",
    "                        if mask is None or not np.any(mask):\n",
    "                            raise ValueError(f\"Loaded mask {mask_path} is empty or invalid\")\n",
    "                        \n",
    "                        # Ensure mask matches frame dimensions\n",
    "                        if mask.shape[:2] != (frame.shape[0], frame.shape[1]):\n",
    "                            mask = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                        logger.debug(f\"Loaded mask for frame {idx}, shape: {mask.shape}, sum: {np.sum(mask)}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to load mask {mask_path}: {str(e)}\")\n",
    "                        mask = None\n",
    "\n",
    "                try:\n",
    "                    processed_frame = self.preprocess_frame(frame_pil, mask, mode)\n",
    "                    frames.append(Image.fromarray(processed_frame))\n",
    "                    masks.append(mask)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Skipping frame {idx} in {video_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            logger.info(f\"Video {os.path.basename(video_path)}: {len(frames)} frames, {fps:.1f} fps\")\n",
    "            logger.info(f\"Mask statistics: {{'total_frames': {len(frames)}, 'frames_with_masks': {sum(1 for m in masks if m is not None)}, 'first_masked_frame': {next((i for i, m in enumerate(masks) if m is not None), None)}}}\")\n",
    "            \n",
    "            if len(frames) == 0:\n",
    "                raise ValueError(f\"No valid frames processed for {video_path}\")\n",
    "            \n",
    "            return frames, masks, os.path.basename(video_path).split('.')[0], fps\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Video processing failed for {video_path}: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def analyze_video(self, video_path: str, prompt: str, mask_dir: Optional[str] = None, mode: str = \"original\"):\n",
    "        \"\"\"\n",
    "        Analyze a single video.\n",
    "        Coordinates frame extraction/masking and LLM generation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting analysis for {video_path} in mode {mode}\")\n",
    "            \n",
    "            # Extract and preprocess frames/masks\n",
    "            frames, masks, video_name, fps = self.process_video(video_path, mask_dir, mode)\n",
    "            \n",
    "            # Check if processing returned any valid frames\n",
    "            if not frames:\n",
    "                logger.error(f\"No frames available for analysis in {video_name}\")\n",
    "                return {\n",
    "                    \"answers\": {\"error\": f\"No valid frames processed for {video_name}\"},\n",
    "                    \"inference_time\": 0.0,\n",
    "                    \"memory_used_mb\": 0.0,\n",
    "                    \"video_name\": video_name,\n",
    "                    \"mode\": mode,\n",
    "                    \"fps\": fps,\n",
    "                    \"frames_processed\": 0\n",
    "                }, frames, masks\n",
    "\n",
    "            # Run inference via the VLM\n",
    "            result = self.generate_description(frames, masks, video_name, prompt, mode)\n",
    "            \n",
    "            # 3. Augment results with metadata\n",
    "            result[\"mode\"] = mode\n",
    "            result[\"fps\"] = fps\n",
    "\n",
    "            return result, frames, masks\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Analysis failed for {video_path}: {str(e)}\")\n",
    "            logger.error(f\"Error details: {traceback.format_exc()}\")\n",
    "            \n",
    "            return {\n",
    "                \"answers\": {\"error\": f\"Error: {str(e)}\"},\n",
    "                \"inference_time\": 0.0,\n",
    "                \"memory_used_mb\": 0.0,\n",
    "                \"video_name\": os.path.basename(video_path).split('.')[0],\n",
    "                \"mode\": mode,\n",
    "                \"fps\": 30.0,\n",
    "                \"frames_processed\": 0\n",
    "            }, [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Main Execution Interface\n",
    "The system provides a dual-mode interface designed for both granular testing and large-scale dataset evaluation.\n",
    "\n",
    "| Feature | Option 1: Manual Sandbox | Option 2: Automated Batch |\n",
    "| :--- | :--- | :--- |\n",
    "| **Primary Use** | Prompt engineering & debugging. | Generating final research metrics. |\n",
    "| **Input** | Manual video ID and custom prompt. | JSON/CSV prompt file. |\n",
    "| **Comparison** | Side-by-side: Original vs. Masked. | Full dataset comparison. |\n",
    "| **Visuals** | Generates real-time attention maps. | Saves batch attention overlays. |\n",
    "| **Output** | Console summary. | Detailed CSV & Summary CSV. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_analysis():\n",
    "    analyzer = VideoAnalyzer()\n",
    "\n",
    "    # Setup Environment\n",
    "    video_files = glob.glob(os.path.join(original_video_dir, \"*.webm\"))\n",
    "    video_names = sorted([os.path.basename(f).split('.')[0] for f in video_files])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VIDEO ANALYSIS SYSTEM\".center(50))\n",
    "    print(\"=\"*50)\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Analyze one video (manual)\")\n",
    "    print(\"2. Automatic prompting and batch analysis\")\n",
    "    print(\"3. Quit\")\n",
    "    choice = input(\"Select option (1-3): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        # --- OPTION 1: SINGLE VIDEO SANDBOX ---\n",
    "        video_name = input(\"Enter video name (e.g., '48970'): \").strip()\n",
    "        if not video_name or video_name not in video_names:\n",
    "            print(f\"Error: Video '{video_name}' not found.\")\n",
    "            return\n",
    "\n",
    "        orig_path = os.path.join(original_video_dir, f\"{video_name}.webm\")\n",
    "        mask_dir = os.path.join(mask_dir_base, video_name, \"masks\")\n",
    "        prompt = input(\"Enter your prompt (as a string): \").strip()\n",
    "\n",
    "        # Handle dictionary prompts (defensive, in case user pastes a dict)\n",
    "        if isinstance(prompt, dict):\n",
    "            prompt = \"\\n\".join(str(v) for v in prompt.values())\n",
    "        \n",
    "        modes = [(\"Original\", orig_path, None, \"original\"), \n",
    "                    (\"Crop-and-mask\", orig_path, mask_dir, \"crop-and-mask\")]\n",
    "        \n",
    "        for mode_name, path, mask_dir, mode_type in modes:\n",
    "            print(f\"\\nProcessing {mode_name} mode...\")\n",
    "            result, frames, masks = analyzer.analyze_video(path, prompt, mask_dir, mode_type)\n",
    "            \n",
    "            answers = result.get('answers', {})\n",
    "            if \"error\" in answers:\n",
    "                print(f\"Error in {mode_name} mode: {answers['error']}\")\n",
    "                continue\n",
    "\n",
    "            # Evaluation\n",
    "            question_list = split_questions(prompt)\n",
    "            question_map = {f\"q{i+1}\": q for i, q in enumerate(question_list)}\n",
    "            metrics = evaluate_responses(video_name, answers, ground_truth, question_map, analyzer.st_model)\n",
    "\n",
    "\n",
    "            # Visualizations & Attention Mapping\n",
    "            analyze_attention_per_mode(\n",
    "                model=analyzer.model,\n",
    "                processor=analyzer.processor,\n",
    "                frames=frames,\n",
    "                video_name=video_name,\n",
    "                mode=mode_type\n",
    "            )\n",
    "            plot_mean_attention_over_frames(\n",
    "                extract_framewise_attention(analyzer.model, frames)\n",
    "            )\n",
    "            save_attention_overlays_for_frames(analyzer.model, frames, video_name, mode_type)\n",
    "\n",
    "            # Optionally print question categories\n",
    "            print(\"Question Categories:\")\n",
    "            for idx, q_text in enumerate(question_list):\n",
    "                category = categorize_question(q_text)\n",
    "                print(f\"  Q{idx+1}: {category}\")\n",
    "\n",
    "            # Print all answers with their metrics\n",
    "            if metrics:\n",
    "                for q_key, q_metrics in metrics.items():\n",
    "                    print(f\"\\nQuestion: {q_metrics.get('question','')}\")\n",
    "                    print(f\"  - Category: {q_metrics.get('category','')}\")\n",
    "                    acc = q_metrics.get('keyword_accuracy', None)\n",
    "                    sim = q_metrics.get('semantic_similarity', None)\n",
    "                    print(f\"  - Accuracy: {acc:.2f}\" if acc is not None else \"  - Accuracy: N/A\")\n",
    "                    print(f\"  - Semantic Similarity: {sim:.2f}\" if sim is not None else \"  - Semantic Similarity: N/A\")\n",
    "                    print(f\"  - Model Answer: {q_metrics.get('model_answer','')}\")\n",
    "                    print(f\"  - Ground Truth: {q_metrics.get('ground_truth','')}\")\n",
    "            else:\n",
    "                for q_key, model_answer in answers.items():\n",
    "                    q_num = int(q_key[1:]) - 1\n",
    "                    question_text = question_list[q_num] if q_num < len(question_list) else \"\"\n",
    "                    category = categorize_question(question_text)\n",
    "                    print(f\"\\nQuestion: {question_text}\")\n",
    "                    print(f\"  - Category: {category}\")\n",
    "                    print(f\"  - Model Answer: {model_answer}\")\n",
    "\n",
    "            # Print total scores\n",
    "            total_acc = sum(q.get('keyword_accuracy',0) for q in metrics.values() if q.get('keyword_accuracy', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('keyword_accuracy', None) is not None)) if metrics else 0.0\n",
    "            total_sim = sum(q.get('semantic_similarity',0) for q in metrics.values() if q.get('semantic_similarity', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('semantic_similarity', None) is not None)) if metrics else 0.0\n",
    "            print(f\"\\nTotal Scores for {mode_name} mode:\")\n",
    "            print(f\"  - Accuracy: {total_acc:.2f}\" if total_acc > 0 else \"  - Accuracy: N/A\")\n",
    "            print(f\"  - Semantic Similarity: {total_sim:.2f}\" if total_sim > 0 else \"  - Semantic Similarity: N/A\")\n",
    "    \n",
    "\n",
    "\n",
    "    elif choice == \"2\":\n",
    "        # --- OPTION 2: AUTOMATIC BATCH ANALYSIS ---\n",
    "        prompts = load_prompts_from_file()\n",
    "        detailed_data = []\n",
    "        accuracy_sums = defaultdict(lambda: {\"accuracy\": [], \"semantic_similarity\": [], \"human_notation\": []})\n",
    "\n",
    "        for video_name, prompt in prompts.items():\n",
    "            orig_path = os.path.join(original_video_dir, f\"{video_name}.webm\")\n",
    "            mask_dir = os.path.join(mask_dir_base, video_name, \"masks\")\n",
    "\n",
    "            if not os.path.exists(orig_path):\n",
    "                print(f\"Video file not found for {video_name}, skipping.\")\n",
    "                continue\n",
    "            if not prompt:\n",
    "                print(f\"Prompt not found for video {video_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(prompt, dict):\n",
    "                prompt = \"\\n\".join(str(v) for v in prompt.values())\n",
    "            modes = [(\"Original\", orig_path, None, \"original\"), (\"Crop-and-mask\", orig_path, mask_dir, \"crop-and-mask\")]\n",
    "            \n",
    "            for mode_name, path, mask_dir, mode_type in modes:\n",
    "                print(\"\\n\" + \"=\"*40)\n",
    "                print(f\"Video: {video_name} | Mode: {mode_type}\")\n",
    "                print(\"=\"*40)\n",
    "                result, frames, masks = analyzer.analyze_video(path, prompt, mask_dir, mode_type)\n",
    "                \n",
    "                # Debug: print raw model response and parsed answers\n",
    "                raw_response = getattr(analyzer, 'last_raw_response', None) if hasattr(analyzer, 'last_raw_response') else None\n",
    "                if raw_response is not None:\n",
    "                    print(f\"[DEBUG] Raw model response for {video_name} ({mode_type}):\\n{raw_response}\\n\")\n",
    "                answers = result.get('answers', {})\n",
    "\n",
    "                print(f\"[DEBUG] Parsed answers for {video_name} ({mode_type}): {answers}\")\n",
    "                if \"error\" in answers:\n",
    "                    print(f\"Error in {mode_name} mode: {answers['error']}\")\n",
    "                    continue\n",
    "\n",
    "                # Print all answers for this video/mode together\n",
    "                print(\"\\nAnswers:\")\n",
    "                question_keys = list(question_map.keys())\n",
    "                for idx, q_key in enumerate(question_keys):\n",
    "                    q_metrics = metrics.get(q_key, {})\n",
    "                    print(f\"Q{idx+1}: {q_metrics.get('question','')}\")\n",
    "                    print(f\"  - Category: {q_metrics.get('category','')}\")\n",
    "                    acc = q_metrics.get('keyword_accuracy', None)\n",
    "                    sim = q_metrics.get('semantic_similarity', None)\n",
    "                    print(f\"  - Accuracy: {acc:.2f}\" if acc is not None else \"  - Accuracy: N/A\")\n",
    "                    print(f\"  - Semantic Similarity: {sim:.2f}\" if sim is not None else \"  - Semantic Similarity: N/A\")\n",
    "                    print(f\"  - Model Answer: {q_metrics.get('model_answer','')}\")\n",
    "                    print(f\"  - Ground Truth: {q_metrics.get('ground_truth','')}\")\n",
    "\n",
    "                    q_metrics = metrics.get(q_key, {})\n",
    "                    acc = q_metrics.get('keyword_accuracy', None)\n",
    "                    sim = q_metrics.get('semantic_similarity', None)\n",
    "                    detailed_data.append({\n",
    "                        'VideoName': video_name,\n",
    "                        'Mode': mode_type,\n",
    "                        'Question': q_metrics.get('question',''),\n",
    "                        'Category': q_metrics.get('category',''),\n",
    "                        'Accuracy': acc if acc is not None else 'N/A',\n",
    "                        'Semantic Similarity': sim if sim is not None else 'N/A',\n",
    "                        'Model Answer': q_metrics.get('model_answer',''),\n",
    "                        'Ground Truth': q_metrics.get('ground_truth',''),\n",
    "                    })\n",
    "\n",
    "                # If metrics is empty (no ground truth), still gather answers\n",
    "                if not metrics and answers:\n",
    "                    for q_key, model_answer in answers.items():\n",
    "                        q_num = int(q_key[1:]) - 1\n",
    "                        question_text = question_list[q_num] if q_num < len(question_list) else \"\"\n",
    "                        category = categorize_question(question_text)\n",
    "                        detailed_data.append({\n",
    "                            'VideoName': video_name,\n",
    "                            'Mode': mode_type,\n",
    "                            'Question': question_text,\n",
    "                            'Category': category,\n",
    "                            'Accuracy': 'N/A',\n",
    "                            'Semantic Similarity': 'N/A',\n",
    "                            'Model Answer': model_answer,\n",
    "                            'Ground Truth': ''\n",
    "                        })\n",
    "\n",
    "                # Aggregate for summary\n",
    "                accuracy_sums[mode_type][\"accuracy\"].append(sum(q.get('keyword_accuracy',0) for q in metrics.values() if q.get('keyword_accuracy', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('keyword_accuracy', None) is not None)) if metrics else 0.0)\n",
    "                accuracy_sums[mode_type][\"semantic_similarity\"].append(sum(q.get('semantic_similarity',0) for q in metrics.values() if q.get('semantic_similarity', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('semantic_similarity', None) is not None)) if metrics else 0.0)\n",
    "                \n",
    "                # Visualize attention maps for this mode\n",
    "                print(f\"\\nSaving attention map visualizations for {mode_name} mode of video {video_name}...\")\n",
    "                if mode_type == \"original\":\n",
    "                    visualize_attention_for_mode(video_name, mode_type, frames)\n",
    "                elif mode_type == \"crop-and-mask\":\n",
    "                    visualize_attention_for_mode(video_name, mode_type, frames, masks)\n",
    "                print(f\"Attention maps saved for {mode_name} mode of video {video_name}.\")\n",
    "        \n",
    "        # Save detailed table\n",
    "        detailed_df = pd.DataFrame(detailed_data)\n",
    "        detailed_df.to_csv(DETAILED_TABLE_PATH, index=False)\n",
    "        print(f\"Detailed evaluation table saved to: {DETAILED_TABLE_PATH}\")\n",
    "        \n",
    "        # Create summary table\n",
    "        summary_data = []\n",
    "        for mode in accuracy_sums:\n",
    "            summary_data.append({\n",
    "                \"Mode\": mode,\n",
    "                \"Accuracy\": sum(accuracy_sums[mode][\"accuracy\"]) / len(accuracy_sums[mode][\"accuracy\"]) if accuracy_sums[mode][\"accuracy\"] else 'N/A',\n",
    "                \"Semantic Similarity\": sum(accuracy_sums[mode][\"semantic_similarity\"]) / len(accuracy_sums[mode][\"semantic_similarity\"]) if accuracy_sums[mode][\"semantic_similarity\"] else 'N/A',\n",
    "                \"Human Notation\": sum(accuracy_sums[mode][\"human_notation\"]) / len(accuracy_sums[mode][\"human_notation\"]) if accuracy_sums[mode][\"human_notation\"] else 'N/A'\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(SUMMARY_TABLE_PATH, index=False)\n",
    "        print(f\"Summary accuracy table saved to: {SUMMARY_TABLE_PATH}\")\n",
    "        print(\"\\nSummary Table:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"Exiting program.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. System Initialization & Entry Point\n",
    "Clean the environment before loading VLM and execute the process of inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        ModelLoader()\n",
    "        interactive_analysis()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProgram terminated by user.\")\n",
    "        sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
