{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import glob\n",
    "import re\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import av\n",
    "import imageio\n",
    "from transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from sentence_transformers.util import cos_sim\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "import sys\n",
    "import traceback\n",
    "import string\n",
    "from typing import List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "original_video_dir = \"/home/kassandralea.briola/Project/VideoCheck\"\n",
    "mask_dir_base = \"/home/kassandralea.briola/Project/output/masks\"\n",
    "model_id = \"llava-hf/llava-next-video-7b-hf\"\n",
    "target_size = 224\n",
    "patch_size = 16\n",
    "max_frames = 24\n",
    "\n",
    "GROUND_TRUTH_PATH = \"/home/kassandralea.briola/Project/GroundTruth/ground_truth_answer.json\"\n",
    "OUTPUT_DIR = \"/home/kassandralea.briola/Project/output\"\n",
    "DETAILED_TABLE_PATH = os.path.join(OUTPUT_DIR, \"detailed_evaluation_table.csv\")\n",
    "SUMMARY_TABLE_PATH = os.path.join(OUTPUT_DIR, \"summary_accuracy_table.csv\")\n",
    "PROMPTS_JSON_PATH = '/home/kassandralea.briola/Project/GroundTruth/prompts.json'\n",
    "\n",
    "\n",
    "# Load ground truth answers\n",
    "try:\n",
    "    with open(GROUND_TRUTH_PATH, 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Ground truth file not found at {GROUND_TRUTH_PATH}\")\n",
    "    ground_truth = {}\n",
    "\n",
    "# --- Prompt Configuration ---\n",
    "def load_prompts_from_file():\n",
    "    try:\n",
    "        with open(PROMPTS_JSON_PATH, 'r') as f:\n",
    "            prompts = json.load(f)\n",
    "        print(f\"Loaded {len(prompts)} prompts from {PROMPTS_JSON_PATH}\")\n",
    "        return prompts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompts.json: {e}\")\n",
    "        return {}\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.lower().translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "\n",
    "def split_questions(prompt_text):\n",
    "    # This regex splits on numbers followed by a dot and space (e.g., \"1. \")\n",
    "    questions = re.split(r'\\d+\\.\\s*', prompt_text)\n",
    "    # Remove any empty strings\n",
    "    questions = [q.strip() for q in questions if q.strip()]\n",
    "    # Remove the instruction part if present (first item)\n",
    "    if len(questions) > 1 and not any(word in questions[0].lower() for word in [\"?\", \"what\", \"how\", \"why\", \"list\"]):\n",
    "        questions = questions[1:]\n",
    "    return questions\n",
    "\n",
    "def categorize_question(question_text):\n",
    "    question_text = question_text.lower().strip()\n",
    "    open_ended_keywords = [\n",
    "        \"why\", \"describe\", \"explain\", \"reason\", \"cause\", \"effect\", \"list\"\n",
    "    ]\n",
    "    # Remove punctuation for matching\n",
    "    question_text_clean = re.sub(r'[^\\w\\s]', '', question_text)\n",
    "    words = question_text_clean.split()\n",
    "    if words and words[0] in open_ended_keywords:\n",
    "        return \"open-ended\"\n",
    "    for keyword in open_ended_keywords:\n",
    "        if keyword in question_text_clean:\n",
    "            return \"open-ended\"\n",
    "    return \"categorical\"\n",
    "\n",
    "# --- Visualization Utility ---\n",
    "def save_attention_overlay(frame, attn_map, save_path, title=None, cmap='jet', alpha=0.5):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    if isinstance(frame, Image.Image):\n",
    "        frame_np = np.array(frame)\n",
    "    else:\n",
    "        frame_np = frame\n",
    "    plt.imshow(frame_np)\n",
    "    if attn_map is not None:\n",
    "        plt.imshow(attn_map, cmap=cmap, alpha=alpha)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_attention_for_mode(video_name, mode, frames, masks=None, output_base=\"attention_maps\"):\n",
    "    mode_dir = os.path.join(output_base, video_name, mode)\n",
    "    os.makedirs(mode_dir, exist_ok=True)\n",
    "    logger.debug(f\"Visualizing attention for {video_name} in mode {mode}, {len(frames)} frames\")\n",
    "    for idx, frame in enumerate(frames):\n",
    "        try:\n",
    "            logger.debug(f\"Processing frame {idx}, frame type: {type(frame)}\")\n",
    "            if mode == \"original\":\n",
    "                attn_map = np.ones(frame.size[::-1]) if isinstance(frame, Image.Image) else np.ones(frame.shape[:2])\n",
    "            elif mode == \"crop-and-mask\":\n",
    "                mask = masks[idx] if masks and idx < len(masks) else None\n",
    "                attn_map = mask if mask is not None else np.zeros(frame.size[::-1] if isinstance(frame, Image.Image) else frame.shape[:2])\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported mode {mode} for visualization\")\n",
    "                continue\n",
    "            save_path = os.path.join(mode_dir, f\"frame_{idx:03d}.png\")\n",
    "            save_attention_overlay(frame, attn_map, save_path, title=f\"{mode} - frame {idx}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to visualize frame {idx} for {video_name} in mode {mode}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# --- Evaluation Functions ---\n",
    "def compute_f1_and_accuracy(model_answer, ground_truth_answer):\n",
    "    model_tokens = set(model_answer.lower().split())\n",
    "    gt_tokens = set(ground_truth_answer.lower().split())\n",
    "    common = model_tokens & gt_tokens\n",
    "    if not common:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        precision = len(common) / len(model_tokens)\n",
    "        recall = len(common) / len(gt_tokens)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return f1\n",
    "\n",
    "def compute_rouge(model_answer, ground_truth_answer):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth_answer, model_answer)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def evaluate_responses(video_id, model_answers, ground_truth, prompt, st_model):\n",
    "    if video_id not in ground_truth or not model_answers:\n",
    "        logger.warning(f\"No ground truth for video {video_id} or no model answers provided\")\n",
    "        return {}\n",
    "    \n",
    "    gt_answers = ground_truth[video_id]\n",
    "    results = {}\n",
    "    for q_key in gt_answers:\n",
    "        question_text = prompt.get(q_key, \"\")\n",
    "        model_ans = model_answers.get(q_key, \"\")\n",
    "        gt_ans = gt_answers[q_key]\n",
    "        category = categorize_question(question_text) \n",
    "        \n",
    "        metrics = {\"category\": category}\n",
    "        \n",
    "        if category == \"categorical\":\n",
    "            model_clean = clean_text(model_ans)\n",
    "            gt_clean = clean_text(gt_ans)\n",
    "            metrics[\"substring_accuracy\"] = int(gt_clean in model_clean)\n",
    "            metrics[\"f1\"] = compute_f1_and_accuracy(model_ans, gt_ans)\n",
    "            metrics[\"rougeL\"] = None\n",
    "            metrics[\"semantic_similarity\"] = None\n",
    "        else:\n",
    "            # Semantic similarity\n",
    "            try:\n",
    "                emb_model = st_model.encode([model_ans.strip(), gt_ans.strip()])\n",
    "                metrics[\"semantic_similarity\"] = util.cos_sim(emb_model[0], emb_model[1]).item()\n",
    "            except Exception:\n",
    "                metrics[\"semantic_similarity\"] = 0.0\n",
    "            # ROUGE-L\n",
    "            try:\n",
    "                metrics[\"rougeL\"] = compute_rouge(model_ans, gt_ans)\n",
    "            except Exception:\n",
    "                metrics[\"rougeL\"] = 0.0\n",
    "            metrics[\"substring_accuracy\"] = None\n",
    "            metrics[\"f1\"] = None\n",
    "        \n",
    "        results[q_key] = {\n",
    "            \"question\": question_text,\n",
    "            \"ground_truth\": gt_ans,\n",
    "            \"model_answer\": model_ans,\n",
    "            **metrics\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance.load_models()\n",
    "        return cls._instance\n",
    "    \n",
    "    def load_models(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        try:\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            logger.info(\"Loading LLaVA-Next-Video model...\")\n",
    "            self.processor = LlavaNextVideoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "            self.model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=quant_config,\n",
    "                device_map={\"\": \"cuda:0\"},\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            self.model.eval()\n",
    "            logger.info(\"Loading SentenceTransformer model...\")\n",
    "            self.st_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "            logger.info(\"All models loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model loading failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAnalyzer:\n",
    "    def __init__(self):\n",
    "        models = ModelLoader()\n",
    "        self.processor = models.processor\n",
    "        self.model = models.model\n",
    "        self.st_model = models.st_model\n",
    "        self.results = []\n",
    "    \n",
    "    def get_frame_indices(self, frame_count: int, max_frames: int = 16) -> List[int]:\n",
    "        if frame_count <= max_frames:\n",
    "            return list(range(frame_count))\n",
    "        step = max(1, frame_count // (max_frames - 3))\n",
    "        indices = list(range(0, frame_count, step))\n",
    "        key_frames = {0, frame_count // 2, frame_count - 1}\n",
    "        indices = sorted(list(set(indices + list(key_frames))))\n",
    "        indices = indices[:max_frames]\n",
    "        logger.debug(f\"Selected frame indices: {indices}\")\n",
    "        return indices\n",
    "    \n",
    "    def compute_motion_intensity(self, prev_frame: np.ndarray, curr_frame: np.ndarray) -> float:\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)\n",
    "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)\n",
    "        diff = cv2.absdiff(prev_gray, curr_gray)\n",
    "        _, diff = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY)\n",
    "        score = np.mean(diff)\n",
    "        logger.debug(f\"Motion intensity for frame: mean diff = {score}\")\n",
    "        return score\n",
    "        \n",
    "    def read_video_pyav(self, video_path: str, max_frames: int = 16) -> tuple[List[np.ndarray], List[int]]:\n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            stream = container.streams.video[0]\n",
    "            total_frames = stream.frames if stream.frames > 0 else None\n",
    "            if total_frames is None:\n",
    "                logger.warning(f\"Video {video_path} reports 0 frames, attempting to count manually\")\n",
    "                total_frames = sum(1 for _ in container.decode(video=0))\n",
    "            indices = np.linspace(0, total_frames - 1, min(max_frames, total_frames), dtype=int)\n",
    "            frames = []\n",
    "            frame_indices = []\n",
    "            container.seek(0)\n",
    "            frame_count = 0\n",
    "            for frame in container.decode(video=0):\n",
    "                if frame_count in indices:\n",
    "                    frame_np = frame.to_ndarray(format='rgb24')\n",
    "                    frames.append(frame_np)\n",
    "                    frame_indices.append(frame_count)\n",
    "                frame_count += 1\n",
    "                if len(frames) >= max_frames:\n",
    "                    break\n",
    "            container.close()\n",
    "            logger.info(f\"Extracted {len(frames)} frames from {video_path}\")\n",
    "            return frames, frame_indices\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read video {video_path}: {str(e)}\")\n",
    "            return [], []\n",
    "    \n",
    "    def read_video_imageio(self, video_path: str, max_frames: int) -> tuple[np.ndarray, List[int]]:\n",
    "        try:\n",
    "            reader = imageio.get_reader(video_path, 'ffmpeg')\n",
    "            total_frames = reader.count_frames()\n",
    "            indices = self.get_frame_indices(total_frames, max_frames)\n",
    "            frames = [reader.get_data(idx) for idx in indices]\n",
    "            reader.close()\n",
    "            logger.info(f\"Imageio extracted {len(frames)} frames from {video_path}\")\n",
    "            return np.stack(frames), indices\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Imageio failed for {video_path}: {str(e)}\")\n",
    "            return np.array([]), []\n",
    "    \n",
    "    def load_frame_masks(self, mask_dir: str, frame_indices: List[int], frame_size: tuple, video_name: str) -> dict:\n",
    "        masks = {}\n",
    "        json_path = os.path.join(mask_dir, \"mask_metadata.json\")\n",
    "        if not os.path.exists(json_path):\n",
    "            logger.error(f\"No mask_metadata.json found in {mask_dir}\")\n",
    "            return {idx: None for idx in frame_indices}\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            available_frames = [int(k) for k in metadata.keys() if metadata[k]]\n",
    "            if not available_frames:\n",
    "                logger.warning(f\"No valid masks found in metadata\")\n",
    "                return {idx: None for idx in frame_indices}\n",
    "            first_mask_frame = min(available_frames)\n",
    "            last_mask_frame = max(available_frames)\n",
    "            logger.info(f\"Masks available from frame {first_mask_frame} to {last_mask_frame}\")\n",
    "            for idx in frame_indices:\n",
    "                if idx < first_mask_frame:\n",
    "                    masks[idx] = None\n",
    "                    logger.debug(f\"Frame {idx} is before first mask frame, setting to None\")\n",
    "                else:\n",
    "                    frame_key = str(idx)\n",
    "                    if frame_key in metadata and metadata[frame_key]:\n",
    "                        try:\n",
    "                            combined_mask = None\n",
    "                            for track in metadata[frame_key]:\n",
    "                                mask_file = os.path.join(mask_dir, track[\"mask_file\"])\n",
    "                                logger.debug(f\"Checking mask file: {mask_file}\")\n",
    "                                if os.path.exists(mask_file):\n",
    "                                    mask = np.load(mask_file)\n",
    "                                    logger.debug(f\"Loaded mask shape: {mask.shape}, dtype: {mask.dtype}, sum: {np.sum(mask)}\")\n",
    "                                    if mask.ndim > 2:\n",
    "                                        mask = mask.squeeze()\n",
    "                                    if mask.shape != frame_size[::-1]:\n",
    "                                        logger.debug(f\"Resizing mask from {mask.shape} to {frame_size[::-1]}\")\n",
    "                                        mask = cv2.resize(mask, frame_size, interpolation=cv2.INTER_NEAREST)\n",
    "                                    mask = (mask > 0).astype(np.uint8)\n",
    "                                    logger.debug(f\"Mask sum after binarization: {np.sum(mask)}\")\n",
    "                                    if np.sum(mask) == 0:\n",
    "                                        logger.warning(f\"Mask {mask_file} is empty after binarization\")\n",
    "                                        continue\n",
    "                                    if combined_mask is None:\n",
    "                                        combined_mask = mask\n",
    "                                    else:\n",
    "                                        combined_mask = np.logical_or(combined_mask, mask).astype(np.uint8)\n",
    "                            masks[idx] = combined_mask\n",
    "                            if combined_mask is not None:\n",
    "                                debug_dir = f\"debug_frames_{video_name}\"\n",
    "                                os.makedirs(debug_dir, exist_ok=True)\n",
    "                                cv2.imwrite(os.path.join(debug_dir, f\"combined_mask_{idx}.png\"), combined_mask * 255)\n",
    "                                logger.debug(f\"Saved combined mask for frame {idx}, shape: {combined_mask.shape}, sum: {np.sum(combined_mask)}\")\n",
    "                            else:\n",
    "                                logger.debug(f\"No valid combined mask for frame {idx}\")\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Error loading mask for frame {idx}: {str(e)}\")\n",
    "                            masks[idx] = None\n",
    "                    else:\n",
    "                        masks[idx] = None\n",
    "                        logger.debug(f\"No metadata or tracks for frame {idx}\")\n",
    "            mask_counts = sum(1 for m in masks.values() if m is not None)\n",
    "            logger.info(f\"Loaded {mask_counts}/{len(frame_indices)} masks successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading mask metadata: {str(e)}\")\n",
    "            return {idx: None for idx in frame_indices}\n",
    "        return masks\n",
    "        \n",
    "    def preprocess_frame(self, frame: Image.Image, mask: Optional[np.ndarray], mode: str) -> np.ndarray:\n",
    "        frame = np.array(frame)\n",
    "        logger.debug(f\"Frame converted to np.ndarray, shape: {frame.shape}\")\n",
    "        if mode == \"crop-and-mask\" and mask is not None:\n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "            coords = cv2.findNonZero(mask)\n",
    "            if coords is not None:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                padding = 10\n",
    "                x1, y1 = max(x - padding, 0), max(y - padding, 0)\n",
    "                x2, y2 = min(x + w + padding, frame.shape[1]), min(y + h + padding, frame.shape[0])\n",
    "                cropped = frame[y1:y2, x1:x2]\n",
    "                logger.debug(f\"Cropped frame to ({y1}:{y2}, {x1}:{x2}), shape: {cropped.shape}\")\n",
    "                cv2.imwrite(f\"debug_frames_3/cropped_mask_{mode}_{self.frame_idx}.png\", cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR))\n",
    "                return cropped\n",
    "            else:\n",
    "                logger.warning(f\"No non-zero mask pixels for frame {self.frame_idx}, returning original frame\")\n",
    "        return frame\n",
    "            \n",
    "    def clean_response(self, response: str) -> dict:\n",
    "        response = re.sub(r'<image>|</image>|<video>|</video>|\\[object.*?\\]|USER:.*?ASSISTANT:|\\bDescribe\\b.*?\\?|What does the hand do.*?\\?', '', response, flags=re.DOTALL)\n",
    "        response = re.sub(r'\\s+', ' ', response).strip()\n",
    "        if not response:\n",
    "            logger.warning(\"No response content after cleaning\")\n",
    "            return {\"error\": \"No valid answers parsed\"}\n",
    "        \n",
    "        logger.debug(f\"Raw response after cleaning: {response}\")\n",
    "        \n",
    "        answers = {}\n",
    "        pattern = r'(\\d+)\\.\\s*([^1-5]\\S.*?)(?=\\s*\\d+\\.\\s*|$)' \n",
    "        matches = re.finditer(pattern, response, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            q_num = match.group(1)\n",
    "            q_answer = match.group(2).strip()\n",
    "            if q_answer:\n",
    "                answers[f\"q{q_num}\"] = q_answer\n",
    "                logger.debug(f\"Parsed answer for q{q_num}: {q_answer}\")\n",
    "        \n",
    "        if not answers:\n",
    "            logger.debug(\"Regex parsing failed, attempting fallback parsing\")\n",
    "            lines = response.split(\"\\n\")\n",
    "            current_q = None\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                match = re.match(r'^(\\d+)\\.\\s*(.+)', line)\n",
    "                if match:\n",
    "                    current_q = f\"q{match.group(1)}\"\n",
    "                    answers[current_q] = match.group(2).strip()\n",
    "                    logger.debug(f\"Fallback: Parsed answer for {current_q}: {answers[current_q]}\")\n",
    "                elif current_q and line:\n",
    "                    answers[current_q] += \" \" + line.strip()\n",
    "                    logger.debug(f\"Fallback: Appended to {current_q}: {answers[current_q]}\")\n",
    "        \n",
    "        if not answers:\n",
    "            logger.warning(f\"Failed to parse response into answers: {response[:100]}...\")\n",
    "            return {\"error\": \"No valid answers parsed\"}\n",
    "        \n",
    "        logger.debug(f\"Final parsed answers: {answers}\")\n",
    "        return answers\n",
    "    \n",
    "    def analyze_response(self, response: str) -> dict:\n",
    "        text = response.lower()\n",
    "        temporal_phrases = [\"moving\", \"then\", \"after\", \"before\", \"while\", \"during\", \"sequence\", \"progress\", \"first\", \"next\", \"finally\"]\n",
    "        causal_phrases = [\"because\", \"causes\", \"leads to\", \"results in\", \"therefore\", \"as a result\", \"due to\", \"makes the\", \"causing\", \"thus\"]\n",
    "        object_pattern = re.compile(r'\\b(a|the|an)\\s+([a-zA-Z]{3,})')\n",
    "        mentioned_objects = set(match[1] for match in object_pattern.finditer(text))\n",
    "        motion_words = [\"moving\", \"sliding\", \"pushing\", \"pulling\", \"shifting\", \"passing\", \"approaching\", \"receding\", \"rotating\"]\n",
    "        return {\n",
    "            \"temporality\": any(phrase in text for phrase in temporal_phrases),\n",
    "            \"causality\": any(phrase in text for phrase in causal_phrases),\n",
    "            \"object_consistency\": len(mentioned_objects),\n",
    "            \"motion_accuracy\": any(word in text for word in motion_words)\n",
    "        }\n",
    "    \n",
    "    def generate_description(self, frames: List[Image.Image], masks: List, video_name: str, prompt: str, mode: str):\n",
    "        torch.cuda.empty_cache()\n",
    "        start_time = time.time()\n",
    "        conversation = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"video\"}\n",
    "            ]\n",
    "        }]\n",
    "        structured_prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        try:\n",
    "            logger.info(f\"Preparing inputs for {video_name} in mode {mode}\")\n",
    "            video_frames = [np.array(frame) for frame in frames]\n",
    "            if mode == \"crop-and-mask\":\n",
    "                max_h = max(f.shape[0] for f in video_frames)\n",
    "                max_w = max(f.shape[1] for f in video_frames)\n",
    "                padded_frames = []\n",
    "                for f in video_frames:\n",
    "                    h, w = f.shape[0], f.shape[1]\n",
    "                    pad_h = max_h - h\n",
    "                    pad_w = max_w - w\n",
    "                    padded = np.pad(f, ((0, pad_h), (0, pad_w), (0,0)), mode='constant', constant_values=0)\n",
    "                    padded_frames.append(padded)\n",
    "                video_frames = padded_frames\n",
    "                logger.info(f\"Padded all crop-and-mask frames to shape ({max_h}, {max_w}, 3)\")\n",
    "            for i, frame in enumerate(video_frames):\n",
    "                logger.debug(f\"Frame {i} shape: {frame.shape}\")\n",
    "            video_tensor = np.stack(video_frames)\n",
    "            video_tensor = video_tensor.transpose(0, 3, 1, 2)\n",
    "            video_tensor = np.expand_dims(video_tensor, 0)\n",
    "            logger.info(f\"Video tensor shape before processing: {video_tensor.shape}\")\n",
    "            inputs = self.processor(\n",
    "                text=structured_prompt,\n",
    "                videos=video_tensor,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                do_rescale=False\n",
    "            )\n",
    "            inputs = {k: v.to(\"cuda:0\") for k, v in inputs.items()}\n",
    "            if 'pixel_values' in inputs:\n",
    "                inputs['pixel_values'] = inputs['pixel_values'].to(dtype=torch.float32)\n",
    "            logger.debug(f\"Input tensor shapes and dtypes: {[(k, v.shape, v.dtype) for k, v in inputs.items()]}\")\n",
    "            \n",
    "            initial_memory = torch.cuda.memory_allocated(\"cuda:0\") / 1024**2\n",
    "            torch.cuda.reset_peak_memory_stats(\"cuda:0\")\n",
    "            \n",
    "            logger.info(f\"Running model inference for {video_name} in mode {mode}\")\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=800\n",
    "                )\n",
    "            \n",
    "            inference_time = time.time() - start_time\n",
    "            peak_memory = torch.cuda.max_memory_allocated(\"cuda:0\") / 1024**2\n",
    "            memory_used = peak_memory - initial_memory\n",
    "            \n",
    "            response = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            answers = self.clean_response(response)\n",
    "            \n",
    "            logger.info(f\"Generated response for {video_name}: {response[:100]}...\")\n",
    "            return {\n",
    "                \"answers\": answers,\n",
    "                \"inference_time\": inference_time,\n",
    "                \"memory_used_mb\": memory_used,\n",
    "                \"video_name\": video_name,\n",
    "                \"fps\": 12.0,\n",
    "                \"frames_processed\": len(frames)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed for {video_name} in mode {mode}: {str(e)}\")\n",
    "            raise\n",
    " \n",
    "    def process_video(self, video_path: str, mask_dir: Optional[str], mode: str) -> Tuple[List[Image.Image], List[Optional[np.ndarray]], float, int]:\n",
    "        try:\n",
    "            logger.info(f\"Attempting to open video: {video_path}\")\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if frame_count <= 0:\n",
    "                logger.warning(f\"Video {video_path} reports 0 frames, attempting to count manually\")\n",
    "                frame_count = 0\n",
    "                while cap.isOpened():\n",
    "                    ret, _ = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame_count += 1\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            logger.info(f\"Extracted {frame_count} frames from {video_path}\")\n",
    "            frames = []\n",
    "            masks = []\n",
    "            self.frame_idx = 0\n",
    "            mask_files = []\n",
    "            if mask_dir and os.path.isdir(mask_dir):\n",
    "                mask_files = sorted(glob.glob(os.path.join(mask_dir, \"*.npy\")))\n",
    "                if mask_files:\n",
    "                    logger.info(f\"Masks available from frame 0 to {len(mask_files)-1}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No .npy mask files found in {mask_dir}. {mode} mode will use original frames.\")\n",
    "            else:\n",
    "                logger.warning(f\"Mask directory {mask_dir} does not exist. {mode} mode will use original frames.\")\n",
    "            \n",
    "            while cap.isOpened() and len(frames) < 24:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_pil = Image.fromarray(frame)\n",
    "                mask = None\n",
    "                if mask_files and self.frame_idx < len(mask_files):\n",
    "                    mask_path = mask_files[self.frame_idx]\n",
    "                    try:\n",
    "                        mask = np.load(mask_path)\n",
    "                        if mask is None or not np.any(mask):\n",
    "                            raise ValueError(f\"Loaded mask {mask_path} is empty or invalid\")\n",
    "                        if mask.shape[:2] != (frame.shape[0], frame.shape[1]):\n",
    "                            mask = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                        logger.debug(f\"Loaded mask for frame {self.frame_idx}, shape: {mask.shape}, sum: {np.sum(mask)}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to load mask {mask_path}: {str(e)}\")\n",
    "                        mask = None\n",
    "                try:\n",
    "                    processed_frame = self.preprocess_frame(frame_pil, mask, mode)\n",
    "                    frames.append(Image.fromarray(processed_frame))\n",
    "                    masks.append(mask)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Skipping frame {self.frame_idx} in {video_path}: {str(e)}\")\n",
    "                    continue\n",
    "                self.frame_idx += 1\n",
    "            cap.release()\n",
    "            logger.info(f\"Video {os.path.basename(video_path)}: {len(frames)} frames, {fps:.1f} fps\")\n",
    "            logger.info(f\"Mask statistics: {{'total_frames': {len(frames)}, 'frames_with_masks': {sum(1 for m in masks if m is not None)}, 'first_masked_frame': {next((i for i, m in enumerate(masks) if m is not None), None)}}}\")\n",
    "            if len(frames) == 0:\n",
    "                raise ValueError(f\"No valid frames processed for {video_path}\")\n",
    "            return frames, masks, os.path.basename(video_path).split('.')[0], fps\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Video processing failed for {video_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def analyze_video(self, video_path: str, prompt: str, mask_dir: Optional[str] = None, mode: str = \"original\"):\n",
    "        try:\n",
    "            logger.info(f\"Starting analysis for {video_path} in mode {mode}\")\n",
    "            frames, masks, video_name, fps = self.process_video(video_path, mask_dir, mode)\n",
    "            if not frames:\n",
    "                logger.error(f\"No frames available for analysis in {video_name}\")\n",
    "                return {\n",
    "                    \"answers\": {\"error\": f\"No valid frames processed for {video_name}\"},\n",
    "                    \"inference_time\": 0.0,\n",
    "                    \"memory_used_mb\": 0.0,\n",
    "                    \"video_name\": video_name,\n",
    "                    \"mode\": mode,\n",
    "                    \"fps\": fps,\n",
    "                    \"frames_processed\": 0\n",
    "                }, frames, masks\n",
    "            result = self.generate_description(frames, masks, video_name, prompt, mode)\n",
    "            result[\"mode\"] = mode\n",
    "            result[\"fps\"] = fps\n",
    "            return result, frames, masks\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Analysis failed for {video_path}: {str(e)}\")\n",
    "            logger.error(f\"Error details: {traceback.format_exc()}\")\n",
    "            return {\n",
    "                \"answers\": {\"error\": f\"Error: {str(e)}\"},\n",
    "                \"inference_time\": 0.0,\n",
    "                \"memory_used_mb\": 0.0,\n",
    "                \"video_name\": os.path.basename(video_path).split('.')[0],\n",
    "                \"mode\": mode,\n",
    "                \"fps\": 30.0,\n",
    "                \"frames_processed\": 0\n",
    "            }, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_analysis():\n",
    "    analyzer = VideoAnalyzer()\n",
    "    video_files = glob.glob(os.path.join(original_video_dir, \"*.webm\"))\n",
    "    video_names = sorted([os.path.basename(f).split('.')[0] for f in video_files])\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VIDEO ANALYSIS SYSTEM\".center(50))\n",
    "    print(\"=\"*50)\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Analyze one video (manual)\")\n",
    "    print(\"2. Automatic prompting and batch analysis\")\n",
    "    print(\"3. Quit\")\n",
    "    choice = input(\"Select option (1-3): \").strip()\n",
    "    if choice == \"1\":\n",
    "        # Manual single video analysis\n",
    "        video_name = input(\"Enter video name (e.g., '48970'): \").strip()\n",
    "        if not video_name or video_name not in video_names:\n",
    "            print(f\"Error: Video '{video_name}' not found.\")\n",
    "            return\n",
    "        orig_path = os.path.join(original_video_dir, f\"{video_name}.webm\")\n",
    "        mask_dir = os.path.join(mask_dir_base, video_name, \"masks\")\n",
    "        prompt = input(\"Enter your prompt (as a string): \").strip()\n",
    "        # Ensure prompt is a string (defensive, in case user pastes a dict)\n",
    "        if isinstance(prompt, dict):\n",
    "            prompt = \"\\n\".join(str(v) for v in prompt.values())\n",
    "        modes = [(\"Original\", orig_path, None, \"original\"), (\"Crop-and-mask\", orig_path, mask_dir, \"crop-and-mask\")]\n",
    "        for mode_name, path, mask_dir, mode_type in modes:\n",
    "            print(f\"\\nProcessing {mode_name} mode...\")\n",
    "            result, frames, masks = analyzer.analyze_video(path, prompt, mask_dir, mode_type)\n",
    "            answers = result.get('answers', {})\n",
    "            if \"error\" in answers:\n",
    "                print(f\"Error in {mode_name} mode: {answers['error']}\")\n",
    "                continue\n",
    "            question_list = split_questions(prompt)\n",
    "            question_map = {f\"q{i+1}\": q for i, q in enumerate(question_list)}\n",
    "            metrics = evaluate_responses(video_name, answers, ground_truth, question_map, analyzer.st_model)\n",
    "            print(f\"\\n{mode_name.upper()} RESULTS for video {video_name}:\")\n",
    "            question_list = split_questions(prompt)\n",
    "            # Print all categories first\n",
    "            print(\"Question Categories:\")\n",
    "            for idx, q_text in enumerate(question_list):\n",
    "                category = categorize_question(q_text)\n",
    "                print(f\"  Q{idx+1}: {category}\")\n",
    "            # Print all answers with their metrics\n",
    "            if metrics:\n",
    "                for q_key, q_metrics in metrics.items():\n",
    "                    print(f\"\\nQuestion: {q_metrics.get('question','')}\")\n",
    "                    print(f\"  - Category: {q_metrics.get('category','')}\")\n",
    "                    acc = q_metrics.get('substring_accuracy', None)\n",
    "                    f1 = q_metrics.get('f1', None)\n",
    "                    rouge = q_metrics.get('rougeL', None)\n",
    "                    sim = q_metrics.get('semantic_similarity', None)\n",
    "                    print(f\"  - Accuracy: {acc:.2f}\" if acc is not None else \"  - Accuracy: N/A\")\n",
    "                    print(f\"  - F1: {f1:.2f}\" if f1 is not None else \"  - F1: N/A\")\n",
    "                    print(f\"  - ROUGE-L: {rouge:.2f}\" if rouge is not None else \"  - ROUGE-L: N/A\")\n",
    "                    print(f\"  - Semantic Similarity: {sim:.2f}\" if sim is not None else \"  - Semantic Similarity: N/A\")\n",
    "                    print(f\"  - Model Answer: {q_metrics.get('model_answer','')}\")\n",
    "                    print(f\"  - Ground Truth: {q_metrics.get('ground_truth','')}\")\n",
    "            else:\n",
    "                for q_key, model_answer in answers.items():\n",
    "                    q_num = int(q_key[1:]) - 1\n",
    "                    question_text = question_list[q_num] if q_num < len(question_list) else \"\"\n",
    "                    category = categorize_question(question_text)\n",
    "                    print(f\"\\nQuestion: {question_text}\")\n",
    "                    print(f\"  - Category: {category}\")\n",
    "                    print(f\"  - Model Answer: {model_answer}\")\n",
    "            # Print total scores\n",
    "            total_acc = sum(q.get('substring_accuracy',0) for q in metrics.values() if q.get('substring_accuracy', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('substring_accuracy', None) is not None)) if metrics else 0.0\n",
    "            total_f1 = sum(q.get('f1',0) for q in metrics.values() if q.get('f1', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('f1', None) is not None)) if metrics else 0.0\n",
    "            total_rouge = sum(q.get('rougeL',0) for q in metrics.values() if q.get('rougeL', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('rougeL', None) is not None)) if metrics else 0.0\n",
    "            total_sim = sum(q.get('semantic_similarity',0) for q in metrics.values() if q.get('semantic_similarity', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('semantic_similarity', None) is not None)) if metrics else 0.0\n",
    "            print(f\"\\nTotal Scores for {mode_name} mode:\")\n",
    "            print(f\"  - Accuracy: {total_acc:.2f}\" if total_acc > 0 else \"  - Accuracy: N/A\")\n",
    "            print(f\"  - F1: {total_f1:.2f}\" if total_f1 > 0 else \"  - F1: N/A\")\n",
    "            print(f\"  - ROUGE-L: {total_rouge:.2f}\" if total_rouge > 0 else \"  - ROUGE-L: N/A\")\n",
    "            print(f\"  - Semantic Similarity: {total_sim:.2f}\" if total_sim > 0 else \"  - Semantic Similarity: N/A\")\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        # Automatic batch analysis with table output\n",
    "        prompts = load_prompts_from_file()\n",
    "        detailed_data = []\n",
    "        accuracy_sums = defaultdict(lambda: {\"accuracy\": [], \"f1\": [], \"rougeL\": [], \"semantic_similarity\": [], \"human_notation\": []})\n",
    "        # Iterate over all videos in the prompt file\n",
    "        for video_name, prompt in prompts.items():\n",
    "            orig_path = os.path.join(original_video_dir, f\"{video_name}.webm\")\n",
    "            mask_dir = os.path.join(mask_dir_base, video_name, \"masks\")\n",
    "            if not os.path.exists(orig_path):\n",
    "                print(f\"Video file not found for {video_name}, skipping.\")\n",
    "                continue\n",
    "            if not prompt:\n",
    "                print(f\"Prompt not found for video {video_name}, skipping.\")\n",
    "                continue\n",
    "            # Ensure prompt is a string (fix for dict prompts from JSON)\n",
    "            if isinstance(prompt, dict):\n",
    "                prompt = \"\\n\".join(str(v) for v in prompt.values())\n",
    "            modes = [(\"Original\", orig_path, None, \"original\"), (\"Crop-and-mask\", orig_path, mask_dir, \"crop-and-mask\")]\n",
    "            for mode_name, path, mask_dir, mode_type in modes:\n",
    "                print(\"\\n\" + \"=\"*40)\n",
    "                print(f\"Video: {video_name} | Mode: {mode_type}\")\n",
    "                print(\"=\"*40)\n",
    "                result, frames, masks = analyzer.analyze_video(path, prompt, mask_dir, mode_type)\n",
    "                # Debug: print raw model response and parsed answers\n",
    "                raw_response = getattr(analyzer, 'last_raw_response', None) if hasattr(analyzer, 'last_raw_response') else None\n",
    "                if raw_response is not None:\n",
    "                    print(f\"[DEBUG] Raw model response for {video_name} ({mode_type}):\\n{raw_response}\\n\")\n",
    "                answers = result.get('answers', {})\n",
    "                print(f\"[DEBUG] Parsed answers for {video_name} ({mode_type}): {answers}\")\n",
    "                if \"error\" in answers:\n",
    "                    print(f\"Error in {mode_name} mode: {answers['error']}\")\n",
    "                    continue\n",
    "                # Split and deduplicate questions\n",
    "                question_list_raw = split_questions(prompt)\n",
    "                seen_questions = set()\n",
    "                question_list = []\n",
    "                for q in question_list_raw:\n",
    "                    if q not in seen_questions:\n",
    "                        question_list.append(q)\n",
    "                        seen_questions.add(q)\n",
    "                # Build question_map with unique questions only\n",
    "                question_map = {f\"q{i+1}\": q for i, q in enumerate(question_list)}\n",
    "                metrics = evaluate_responses(video_name, answers, ground_truth, question_map, analyzer.st_model)\n",
    "                # Print all answers for this video/mode together\n",
    "                print(\"\\nAnswers:\")\n",
    "                question_keys = list(question_map.keys())\n",
    "                for idx, q_key in enumerate(question_keys):\n",
    "                    q_metrics = metrics.get(q_key, {})\n",
    "                    print(f\"Q{idx+1}: {q_metrics.get('question','')}\")\n",
    "                    print(f\"  - Category: {q_metrics.get('category','')}\")\n",
    "                    acc = q_metrics.get('substring_accuracy', None)\n",
    "                    f1 = q_metrics.get('f1', None)\n",
    "                    rouge = q_metrics.get('rougeL', None)\n",
    "                    sim = q_metrics.get('semantic_similarity', None)\n",
    "                    print(f\"  - Accuracy: {acc:.2f}\" if acc is not None else \"  - Accuracy: N/A\")\n",
    "                    print(f\"  - F1: {f1:.2f}\" if f1 is not None else \"  - F1: N/A\")\n",
    "                    print(f\"  - ROUGE-L: {rouge:.2f}\" if rouge is not None else \"  - ROUGE-L: N/A\")\n",
    "                    print(f\"  - Semantic Similarity: {sim:.2f}\" if sim is not None else \"  - Semantic Similarity: N/A\")\n",
    "                    print(f\"  - Model Answer: {q_metrics.get('model_answer','')}\")\n",
    "                    print(f\"  - Ground Truth: {q_metrics.get('ground_truth','')}\")\n",
    "                # Prompt for annotation for each question after all are printed\n",
    "                for idx, q_key in enumerate(question_keys):\n",
    "                    while True:\n",
    "                        human_input = input(f\"Human Notation for Q{idx+1} (1=correct, 0=incorrect): \").strip()\n",
    "                        if human_input in ['0', '1', '0.5']:\n",
    "                            human_notation = float(human_input)\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Please enter 1 for correct or 0 for incorrect.\")\n",
    "                    q_metrics = metrics.get(q_key, {})\n",
    "                    acc = q_metrics.get('substring_accuracy', None)\n",
    "                    f1 = q_metrics.get('f1', None)\n",
    "                    rouge = q_metrics.get('rougeL', None)\n",
    "                    sim = q_metrics.get('semantic_similarity', None)\n",
    "                    detailed_data.append({\n",
    "                        'VideoName': video_name,\n",
    "                        'Mode': mode_type,\n",
    "                        'Question': q_metrics.get('question',''),\n",
    "                        'Category': q_metrics.get('category',''),\n",
    "                        'Accuracy': acc if acc is not None else 'N/A',\n",
    "                        'F1': f1 if f1 is not None else 'N/A',\n",
    "                        'ROUGE-L': rouge if rouge is not None else 'N/A',\n",
    "                        'Semantic Similarity': sim if sim is not None else 'N/A',\n",
    "                        'Model Answer': q_metrics.get('model_answer',''),\n",
    "                        'Ground Truth': q_metrics.get('ground_truth',''),\n",
    "                        'Human Notation': human_notation\n",
    "                    })\n",
    "                # If metrics is empty (no ground truth), still gather answers\n",
    "                if not metrics and answers:\n",
    "                    for q_key, model_answer in answers.items():\n",
    "                        q_num = int(q_key[1:]) - 1\n",
    "                        question_text = question_list[q_num] if q_num < len(question_list) else \"\"\n",
    "                        category = categorize_question(question_text)\n",
    "                        detailed_data.append({\n",
    "                            'VideoName': video_name,\n",
    "                            'Mode': mode_type,\n",
    "                            'Question': question_text,\n",
    "                            'Category': category,\n",
    "                            'Accuracy': 'N/A',\n",
    "                            'F1': 'N/A',\n",
    "                            'ROUGE-L': 'N/A',\n",
    "                            'Semantic Similarity': 'N/A',\n",
    "                            'Model Answer': model_answer,\n",
    "                            'Ground Truth': ''\n",
    "                        })\n",
    "                    accuracy_sums[mode_type][\"human_notation\"].append(human_notation)\n",
    "                # Aggregate for summary\n",
    "                accuracy_sums[mode_type][\"accuracy\"].append(sum(q.get('substring_accuracy',0) for q in metrics.values() if q.get('substring_accuracy', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('substring_accuracy', None) is not None)) if metrics else 0.0)\n",
    "                accuracy_sums[mode_type][\"f1\"].append(sum(q.get('f1',0) for q in metrics.values() if q.get('f1', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('f1', None) is not None)) if metrics else 0.0)\n",
    "                accuracy_sums[mode_type][\"rougeL\"].append(sum(q.get('rougeL',0) for q in metrics.values() if q.get('rougeL', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('rougeL', None) is not None)) if metrics else 0.0)\n",
    "                accuracy_sums[mode_type][\"semantic_similarity\"].append(sum(q.get('semantic_similarity',0) for q in metrics.values() if q.get('semantic_similarity', None) is not None) / max(1, sum(1 for q in metrics.values() if q.get('semantic_similarity', None) is not None)) if metrics else 0.0)\n",
    "                # Visualize attention maps for this mode\n",
    "                print(f\"\\nSaving attention map visualizations for {mode_name} mode of video {video_name}...\")\n",
    "                if mode_type == \"original\":\n",
    "                    visualize_attention_for_mode(video_name, mode_type, frames)\n",
    "                elif mode_type == \"crop-and-mask\":\n",
    "                    visualize_attention_for_mode(video_name, mode_type, frames, masks)\n",
    "                print(f\"Attention maps saved for {mode_name} mode of video {video_name}.\")\n",
    "        # Save detailed table\n",
    "        detailed_df = pd.DataFrame(detailed_data)\n",
    "        detailed_df.to_csv(DETAILED_TABLE_PATH, index=False)\n",
    "        print(f\"Detailed evaluation table saved to: {DETAILED_TABLE_PATH}\")\n",
    "        # Create summary table\n",
    "        summary_data = []\n",
    "        for mode in accuracy_sums:\n",
    "            summary_data.append({\n",
    "                \"Mode\": mode,\n",
    "                \"Accuracy\": sum(accuracy_sums[mode][\"accuracy\"]) / len(accuracy_sums[mode][\"accuracy\"]) if accuracy_sums[mode][\"accuracy\"] else 'N/A',\n",
    "                \"F1\": sum(accuracy_sums[mode][\"f1\"]) / len(accuracy_sums[mode][\"f1\"]) if accuracy_sums[mode][\"f1\"] else 'N/A',\n",
    "                \"ROUGE-L\": sum(accuracy_sums[mode][\"rougeL\"]) / len(accuracy_sums[mode][\"rougeL\"]) if accuracy_sums[mode][\"rougeL\"] else 'N/A',\n",
    "                \"Semantic Similarity\": sum(accuracy_sums[mode][\"semantic_similarity\"]) / len(accuracy_sums[mode][\"semantic_similarity\"]) if accuracy_sums[mode][\"semantic_similarity\"] else 'N/A',\n",
    "                \"Human Notation\": sum(accuracy_sums[mode][\"human_notation\"]) / len(accuracy_sums[mode][\"human_notation\"]) if accuracy_sums[mode][\"human_notation\"] else 'N/A'\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(SUMMARY_TABLE_PATH, index=False)\n",
    "        print(f\"Summary accuracy table saved to: {SUMMARY_TABLE_PATH}\")\n",
    "        print(\"\\nSummary Table:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"Exiting program.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        ModelLoader()\n",
    "        interactive_analysis()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProgram terminated by user.\")\n",
    "        sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
