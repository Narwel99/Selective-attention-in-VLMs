{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up\n",
    "Download all neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import json\n",
    "import imageio\n",
    "from IPython.display import display, HTML, Image\n",
    "import base64\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurate all paths and parameters (setup class)\n",
    "-- devices \n",
    "-- SAM model path \n",
    "-- Parameters \n",
    "-- outputs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Device settings\n",
    "    DEVICE = 'cuda:0' if torch.cuda.is_available() and torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "    # Model paths\n",
    "    SAM_MODEL_TYPE = 'vit_h'\n",
    "    SAM_CHECKPOINT = '/home/kassandralea.briola/Project/segment-anything/segment-anything/sam_vit_h_4b8939.pth'\n",
    "\n",
    "\n",
    "    # Selection and tracking parameters\n",
    "    MANUAL_SAMPLE_INTERVAL = 10  # Sample every Nth frame for manual selection\n",
    "    MIN_TRACK_LENGTH = 5         # Minimum frames an object should be tracked\n",
    "    MIN_BOX_AREA = 50            # Minimum area for valid bounding box (pixels)\n",
    "    MAX_BOX_AREA = 100000        # Maximum area for valid bounding box (pixels)\n",
    "    MIN_BOX_RATIO = 0.01         # Minimum width/height ratio for valid boxes\n",
    "    MAX_BOX_RATIO = 10.0         # Maximum width/height ratio for valid boxes\n",
    "    GRID_STEP = 50               # Pixel interval for coordinate grid\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_paths(output_base_dir):\n",
    "        Config.OUTPUT_BASE_DIR = output_base_dir\n",
    "        Config.verify_paths()\n",
    "\n",
    "    # Directory for outputs\n",
    "    @staticmethod\n",
    "    def get_video_dirs(video_name, create=True):\n",
    "        video_dirs = {\n",
    "            'base': Config.OUTPUT_BASE_DIR,\n",
    "            'frames': os.path.join(Config.OUTPUT_BASE_DIR, 'frames', video_name, 'frames'),\n",
    "            'masks': os.path.join(Config.OUTPUT_BASE_DIR, 'masks', video_name, 'masks'),\n",
    "            'segmented': os.path.join(Config.OUTPUT_BASE_DIR, 'boxes', video_name, 'frames_with_boxes'),\n",
    "            'segmented_only': os.path.join(Config.OUTPUT_BASE_DIR, 'segmented_only', video_name, 'frames'), \n",
    "            'videos': os.path.join(Config.OUTPUT_BASE_DIR, 'videos', video_name),\n",
    "            'tracking_results': os.path.join(Config.OUTPUT_BASE_DIR, 'tracking_results', video_name),\n",
    "            'selections': os.path.join(Config.OUTPUT_BASE_DIR, 'selections', video_name)\n",
    "        }\n",
    "        if create:\n",
    "            for dir_path in video_dirs.values():\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "        return video_dirs\n",
    "\n",
    "    @staticmethod\n",
    "    def verify_paths():\n",
    "        if not Config.OUTPUT_BASE_DIR:\n",
    "            raise ValueError(\"OUTPUT_BASE_DIR not set. Call setup_paths first.\")\n",
    "        os.makedirs(Config.OUTPUT_BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manages user input, visual feedback, and stores the selected coordinates and boxes  \n",
    "Enables interactive manual object's selection in video frames for object detection, tracking and segmentation  \n",
    "interface display each frame with a coordinate grid  \n",
    "user adds or remove points from currend frame\n",
    "SAM model generate boundign boxes and assign IDs to object and generate sementation masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualCoordinateSelector:\n",
    "    def __init__(self, sam_predictor=None):\n",
    "        self.boxes = {}  \n",
    "        self.points = {}  \n",
    "        self.obj_id = 1  # Unique ID for each object\n",
    "        self.selected_objects = set()  # Track selected obj_ids\n",
    "        self.sam_predictor = sam_predictor\n",
    "        self.last_frame_idx = None\n",
    "\n",
    "    def _add_coordinate_grid(self, frame, frame_idx):\n",
    "        grid_frame = frame.copy()\n",
    "        height, width = grid_frame.shape[:2]\n",
    "        step = Config.GRID_STEP\n",
    "        for x in range(0, width, step):\n",
    "            cv2.line(grid_frame, (x, 0), (x, height), (128, 128, 128), 1)\n",
    "            cv2.putText(grid_frame, str(x), (x, 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        for y in range(0, height, step):\n",
    "            cv2.line(grid_frame, (0, y), (width, y), (128, 128, 128), 1)\n",
    "            cv2.putText(grid_frame, str(y), (5, y+10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        if frame_idx in self.points:\n",
    "            for point in self.points[frame_idx]:\n",
    "                x, y, obj_id = point\n",
    "                cv2.circle(grid_frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                label = f\"ID:{obj_id}\"\n",
    "                cv2.putText(grid_frame, label, (int(x), int(y)-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        if frame_idx in self.boxes:\n",
    "            for box in self.boxes[frame_idx]:\n",
    "                x1, y1, x2, y2, obj_id = box\n",
    "                cv2.rectangle(grid_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        return grid_frame\n",
    "\n",
    "    def _display_frame(self, frame):\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        _, buffer = cv2.imencode('.jpg', frame_rgb)\n",
    "        img_str = base64.b64encode(buffer).decode('utf-8')\n",
    "        display(Image(data=base64.b64decode(img_str)))\n",
    "\n",
    "    def get_user_selections(self, video_path, cap, reader, total_frames):\n",
    "        self.boxes = {}\n",
    "        self.points = {}\n",
    "        self.obj_id = 1\n",
    "        self.selected_objects = set()\n",
    "        self.last_frame_idx = None\n",
    "\n",
    "        if self.sam_predictor is None:\n",
    "            print(\"⚠️ SAM predictor not initialized. Cannot select objects.\")\n",
    "            return None\n",
    "\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            if cap is not None and cap.isOpened():\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret or frame is None:\n",
    "                    print(f\"⚠️ Failed to read frame {frame_idx}\")\n",
    "                    return None\n",
    "            elif reader is not None:\n",
    "                try:\n",
    "                    frame = reader.get_data(frame_idx)\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Failed to read frame {frame_idx}: {str(e)}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"⚠️ No valid video reader\")\n",
    "                return None\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            grid_frame = self._add_coordinate_grid(frame, frame_idx)\n",
    "            print(f\"\\n=== Frame {frame_idx}/{total_frames-1} ===\")\n",
    "            print(f\"Frame shape: {frame.shape} (width: {frame.shape[1]}, height: {frame.shape[0]})\")\n",
    "            self._display_frame(grid_frame)\n",
    "\n",
    "            display(HTML(f\"<b>Frame {frame_idx}: Select points for new objects or navigate</b>\"))\n",
    "            while True:\n",
    "                print(\"\\nOptions:\")\n",
    "                print(\"  a: Add a point for a new object\")\n",
    "                print(\"  r: Remove the last point added in this frame\")\n",
    "                print(f\"  n: Move to next frame (step {Config.MANUAL_SAMPLE_INTERVAL})\")\n",
    "                print(f\"  f: Move to a specific frame (0-{total_frames-1})\")\n",
    "                print(\"  j: Jump to a different video\") \n",
    "                print(\"  s: Skip this video\")\n",
    "                print(\"  t: Terminate all processing\")\n",
    "                print(\"  q: Finish selection and save for this video\")\n",
    "                sys.stdout.flush()\n",
    "                try:\n",
    "                    choice = input(\"Enter choice (a, r, n, f, j, s, t, q): \").strip().lower()\n",
    "                    print(f\"User entered choice: '{choice}'\")\n",
    "                    sys.stdout.flush()\n",
    "                except (EOFError, KeyboardInterrupt):\n",
    "                    print(\"⚠️ Input interrupted. Please try again.\")\n",
    "                    continue\n",
    "\n",
    "                if choice == 'a':\n",
    "                    try:\n",
    "                        coords = input(\"Enter x y coordinates (e.g., '230 100'): \").strip()\n",
    "                        print(f\"User entered coordinates: '{coords}'\")\n",
    "                        sys.stdout.flush()\n",
    "                        parts = [p for p in re.split(r'\\s+', coords.strip()) if p]\n",
    "                        if len(parts) != 2:\n",
    "                            print(\"⚠️ Invalid input. Enter exactly two numbers separated by a space (e.g., '230 100').\")\n",
    "                            continue\n",
    "                        try:\n",
    "                            x, y = map(float, parts)\n",
    "                        except ValueError:\n",
    "                            print(\"⚠️ Invalid coordinates. Both x and y must be numbers (e.g., '230 100').\")\n",
    "                            continue\n",
    "\n",
    "                        if not (0 <= x < frame.shape[1] and 0 <= y < frame.shape[0]):\n",
    "                            print(f\"⚠️ Point ({x}, {y}) is outside frame boundaries (width: 0-{frame.shape[1]-1}, height: 0-{frame.shape[0]-1}).\")\n",
    "                            continue\n",
    "\n",
    "                        if frame_idx != self.last_frame_idx:\n",
    "                            self.sam_predictor.set_image(frame_rgb)\n",
    "                            self.last_frame_idx = frame_idx\n",
    "                        masks, scores, _ = self.sam_predictor.predict(\n",
    "                            point_coords=np.array([[x, y]]),\n",
    "                            point_labels=np.array([1]),\n",
    "                            multimask_output=True\n",
    "                        )\n",
    "                        best_mask_idx = np.argmax(scores)\n",
    "                        mask = masks[best_mask_idx]\n",
    "                        coords = np.where(mask)\n",
    "                        if len(coords[0]) == 0:\n",
    "                            print(\"⚠️ No valid mask generated for this point. Try a different point.\")\n",
    "                            continue\n",
    "                        x1, y1 = int(np.min(coords[1])), int(np.min(coords[0]))\n",
    "                        x2, y2 = int(np.max(coords[1])), int(np.max(coords[0]))\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        ratio = (x2 - x1) / (y2 - y1) if (y2 - y1) > 0 else float('inf')\n",
    "                        if not (Config.MIN_BOX_AREA <= area <= Config.MAX_BOX_AREA and\n",
    "                                Config.MIN_BOX_RATIO <= ratio <= Config.MAX_BOX_RATIO):\n",
    "                            print(f\"⚠️ Invalid box generated: area={area}, ratio={ratio}. Try a different point.\")\n",
    "                            continue\n",
    "\n",
    "                        if frame_idx not in self.points:\n",
    "                            self.points[frame_idx] = []\n",
    "                        if frame_idx not in self.boxes:\n",
    "                            self.boxes[frame_idx] = []\n",
    "                        self.points[frame_idx].append([float(x), float(y), self.obj_id])\n",
    "                        self.boxes[frame_idx].append([x1, y1, x2, y2, self.obj_id])\n",
    "                        print(f\"✅ Added point ID:{self.obj_id} at ({x:.1f}, {y:.1f}), box: ({x1:.1f}, {y1:.1f})-({x2:.1f}, {y2:.1f})\")\n",
    "                        sys.stdout.flush()\n",
    "                        self._display_frame(self._add_coordinate_grid(frame, frame_idx))\n",
    "                        self.selected_objects.add(self.obj_id)\n",
    "                        self.obj_id += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Error processing point: {str(e)}. Please try again.\")\n",
    "                        sys.stdout.flush()\n",
    "                elif choice == 'r':\n",
    "                    if frame_idx in self.points and self.points[frame_idx]:\n",
    "                        removed_point = self.points[frame_idx].pop()\n",
    "                        self.boxes[frame_idx].pop()\n",
    "                        removed_obj_id = removed_point[2]\n",
    "                        self.selected_objects.discard(removed_obj_id)\n",
    "                        self.obj_id = max(self.selected_objects, default=0) + 1\n",
    "                        if not self.points[frame_idx]:\n",
    "                            del self.points[frame_idx]\n",
    "                            del self.boxes[frame_idx]\n",
    "                        print(f\"✅ Removed point ID:{removed_obj_id}\")\n",
    "                        self._display_frame(self._add_coordinate_grid(frame, frame_idx))\n",
    "                    else:\n",
    "                        print(\"⚠️ No points to remove in this frame.\")\n",
    "                    sys.stdout.flush()\n",
    "                elif choice == 'n':\n",
    "                    frame_idx = min(frame_idx + Config.MANUAL_SAMPLE_INTERVAL, total_frames - 1)\n",
    "                    print(f\"Moving to frame {frame_idx}\")\n",
    "                    sys.stdout.flush()\n",
    "                    break\n",
    "                elif choice == 'f':\n",
    "                    try:\n",
    "                        new_idx = input(f\"Enter frame number (0-{total_frames-1}): \").strip()\n",
    "                        print(f\"User entered frame: '{new_idx}'\")\n",
    "                        sys.stdout.flush()\n",
    "                        new_idx = int(new_idx)\n",
    "                        if 0 <= new_idx < total_frames:\n",
    "                            frame_idx = new_idx\n",
    "                            print(f\"Moving to frame {frame_idx}\")\n",
    "                            sys.stdout.flush()\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"⚠️ Frame number out of range. Enter an integer between 0 and {total_frames-1}.\")\n",
    "                            sys.stdout.flush()\n",
    "                    except ValueError:\n",
    "                        print(\"⚠️ Invalid input. Frame number must be an integer.\")\n",
    "                        sys.stdout.flush()\n",
    "                # In get_user_selections():\n",
    "                elif choice == 'j':  # Jump to video option\n",
    "                    print(f\"\\nCurrent video: {os.path.splitext(os.path.basename(video_path))[0]}\")\n",
    "                    return {'action': 'jump'}\n",
    "                elif choice == 's':\n",
    "                    print(f\"✅ Skipping video {os.path.basename(video_path)}\")\n",
    "                    sys.stdout.flush()\n",
    "                    return {'action': 'skip'}\n",
    "                elif choice == 't':\n",
    "                    print(\"✅ Terminating all processing as requested by user.\")\n",
    "                    sys.stdout.flush()\n",
    "                    raise SystemExit(\"Terminated by user request\")\n",
    "                elif choice == 'q':\n",
    "                    if not self.boxes:\n",
    "                        print(\"⚠️ No selections made. Select at least one point before exiting, or use 's' to skip.\")\n",
    "                        sys.stdout.flush()\n",
    "                        continue\n",
    "                    print(\"✅ Saving selections and exiting selection phase.\")\n",
    "                    sys.stdout.flush()\n",
    "                    return self.boxes\n",
    "                else:\n",
    "                    print(f\"⚠️ Invalid choice '{choice}'. Please enter 'a', 'r', 'n', 'f', 's', 't', 'j' or 'q'.\")\n",
    "                    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializes SAM (Segment-anything) model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    def __init__(self, sam_checkpoint=Config.SAM_CHECKPOINT, sam_model_type=Config.SAM_MODEL_TYPE, device=Config.DEVICE):\n",
    "        self.sam = None\n",
    "        self.device = device\n",
    "        self.load_sam(sam_checkpoint, sam_model_type)\n",
    "\n",
    "    def load_sam(self, checkpoint_path, model_type):\n",
    "        print(f\"Loading SAM model: type={model_type}, checkpoint={checkpoint_path}, device={self.device}\")\n",
    "        try:\n",
    "            if checkpoint_path is None or not os.path.exists(checkpoint_path):\n",
    "                raise FileNotFoundError(f\"SAM checkpoint not found at {checkpoint_path}\")\n",
    "            if not torch.cuda.is_available() and self.device.startswith('cuda'):\n",
    "                print(\"⚠️ CUDA not available, falling back to CPU\")\n",
    "                self.device = 'cpu'\n",
    "            sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "            sam.to(device=self.device)\n",
    "            self.sam = SamPredictor(sam)\n",
    "            print(f\"✅ SAM model ({model_type}) loaded on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load SAM: {str(e)}\")\n",
    "            self.sam = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track invidual object and store its ID,bouding boxe, confidence score and last known position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackWrapper:\n",
    "    def __init__(self, obj_id, bbox, conf, last_position=None):\n",
    "        self.obj_id = obj_id\n",
    "        self.bbox = bbox  # [x1, y1, x2, y2]\n",
    "        self.conf = conf\n",
    "        self.last_position = last_position  # (cx, cy) for occlusion handling\n",
    "\n",
    "    def update(self, bbox, conf, last_position=None):\n",
    "        self.bbox = bbox\n",
    "        self.conf = conf\n",
    "        self.last_position = last_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video processing  \n",
    "-extract frames\n",
    "-handle manual selection  \n",
    "-track object accross frames  \n",
    "-apply segmentation  \n",
    "-visualize results  \n",
    "-save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoProcessor:\n",
    "    def __init__(self, video_dirs, model_manager, current_video):\n",
    "        self.video_dirs = video_dirs\n",
    "        self.model_manager = model_manager\n",
    "        self.current_video = current_video\n",
    "        self.selections = {}\n",
    "        self.manual_tracks = {}\n",
    "        self.track_history = {}\n",
    "        self.color_map = {}\n",
    "\n",
    "    def _get_color(self, obj_id):\n",
    "        if obj_id not in self.color_map:\n",
    "            self.color_map[obj_id] = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "        return self.color_map[obj_id]\n",
    "    \n",
    "    def enhance_segmentation(self, frame, mask, bbox):\n",
    "            \"\"\"Enhance the segmentation mask and refine the bounding box.\"\"\"\n",
    "            import cv2\n",
    "            import numpy as np\n",
    "            # Morphological smoothing\n",
    "            kernel = np.ones((3,3), np.uint8)\n",
    "            mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "            mask = cv2.erode(mask, kernel, iterations=1)\n",
    "            \n",
    "            # Refine bbox based on updated mask\n",
    "            coords = np.where(mask)\n",
    "            if len(coords[0]) > 0:\n",
    "                x1, y1 = int(np.min(coords[1])), int(np.min(coords[0]))\n",
    "                x2, y2 = int(np.max(coords[1])), int(np.max(coords[0]))\n",
    "                return [x1, y1, x2, y2], mask\n",
    "            return bbox, mask\n",
    "\n",
    "    def _visualize_results(self, frame, tracks, masks):\n",
    "        \"\"\"Original frame with segmentation overlay and bounding boxes\"\"\"\n",
    "        vis_frame = frame.copy()  # Keep original frame intact\n",
    "        for track, mask in zip(tracks, masks):\n",
    "            if mask is not None:\n",
    "                color = self._get_color(track.obj_id)\n",
    "                # Create transparent overlay\n",
    "                overlay = np.zeros_like(frame)\n",
    "                overlay[mask == 1] = color\n",
    "                vis_frame = cv2.addWeighted(overlay, 0.5, vis_frame, 1.0, 0)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                x1, y1, x2, y2 = map(int, track.bbox)\n",
    "                cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                label = f\"ID:{track.obj_id} ({track.conf:.2f})\"\n",
    "                cv2.putText(vis_frame, label, (x1, y1-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "        return vis_frame\n",
    "\n",
    "    def _visualize_segmented_only(self, frame, tracks, masks):\n",
    "        \"\"\"Original frame with only segmentation overlay (no boxes/labels)\"\"\"\n",
    "        vis_frame = frame.copy()  # Keep original frame intact\n",
    "        for track, mask in zip(tracks, masks):\n",
    "            if mask is not None:\n",
    "                color = self._get_color(track.obj_id)\n",
    "                # Create transparent overlay\n",
    "                overlay = np.zeros_like(frame)\n",
    "                overlay[mask == 1] = color\n",
    "                vis_frame = cv2.addWeighted(overlay, 0.5, vis_frame, 1.0, 0)\n",
    "        return vis_frame\n",
    "\n",
    "    def extract_frames(self, video_path):\n",
    "        os.makedirs(self.video_dirs['frames'], exist_ok=True)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"⚠️ Failed to open video {video_path}\")\n",
    "            return\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_path = os.path.join(self.video_dirs['frames'], f\"frame_{frame_idx:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            frame_idx += 1\n",
    "        cap.release()\n",
    "        print(f\"Extracted {frame_idx} frames to {self.video_dirs['frames']}\")\n",
    "\n",
    "    def reconstruct_video(self, frame_dir, output_path, fps):\n",
    "        frame_files = sorted([f for f in os.listdir(frame_dir) if f.endswith(('.jpg', '.png'))])\n",
    "        if not frame_files:\n",
    "            print(f\"No frames found in {frame_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Get frame dimensions from first frame\n",
    "        first_frame = cv2.imread(os.path.join(frame_dir, frame_files[0]))\n",
    "        if first_frame is None:\n",
    "            print(\"Failed to read first frame\")\n",
    "            return\n",
    "        height, width = first_frame.shape[:2]\n",
    "        \n",
    "        # WEBM writer configuration\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'VP80')  # VP8 codec for WEBM\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        if not out.isOpened():\n",
    "            print(\"Failed to create video writer\")\n",
    "            return\n",
    "        \n",
    "        for frame_file in tqdm(frame_files, desc=\"Reconstructing video\"):\n",
    "            frame_path = os.path.join(frame_dir, frame_file)\n",
    "            frame = cv2.imread(frame_path)\n",
    "            if frame is None:\n",
    "                continue\n",
    "                \n",
    "            # Write original frame (with any overlays already applied during visualization)\n",
    "            out.write(frame)\n",
    "        \n",
    "        out.release()\n",
    "        print(f\"Video saved to {output_path}\")\n",
    "\n",
    "    def manual_selection_phase(self, video_path):\n",
    "        print(\"\\n=== PHASE 1: Manual Object Selection ===\")\n",
    "        if self.model_manager.sam is None:\n",
    "            print(\" SAM predictor not initialized. Cannot proceed with manual selection.\")\n",
    "            return None\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        reader = None\n",
    "        if not cap.isOpened():\n",
    "            try:\n",
    "                reader = imageio.get_reader(video_path)\n",
    "                print(\"Using imageio fallback reader\")\n",
    "            except Exception as e:\n",
    "                print(f\" Imageio fallback failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.isOpened() else len(reader)\n",
    "        selector = ManualCoordinateSelector(self.model_manager.sam)\n",
    "        selection_result = selector.get_user_selections(video_path, cap, reader, total_frames)\n",
    "\n",
    "        if cap.isOpened():\n",
    "            cap.release()\n",
    "        if reader is not None:\n",
    "            reader.close()\n",
    "\n",
    "        # Handle special actions (jump/skip)\n",
    "        if isinstance(selection_result, dict) and 'action' in selection_result:\n",
    "            return selection_result\n",
    "\n",
    "        if not selection_result:\n",
    "            print(\" No selections made\")\n",
    "            return None\n",
    "\n",
    "        # Only process selections if we got actual boxes\n",
    "        self.selections = {int(k): v for k, v in selection_result.items()}\n",
    "        for frame_idx, boxes in self.selections.items():\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, obj_id = box\n",
    "                self.manual_tracks[obj_id] = {\n",
    "                    'first_frame': int(frame_idx),\n",
    "                    'first_box': [x1, y1, x2, y2],\n",
    "                    'first_point': None\n",
    "                }\n",
    "                if frame_idx in selector.points:\n",
    "                    for point in selector.points[frame_idx]:\n",
    "                        if point[2] == obj_id:\n",
    "                            self.manual_tracks[obj_id]['first_point'] = point[:2]\n",
    "                            break\n",
    "\n",
    "        os.makedirs(self.video_dirs['selections'], exist_ok=True)\n",
    "        output_file = os.path.join(self.video_dirs['selections'], 'manual_selections.json')\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(self.selections, f)\n",
    "        print(f\" Saved manual selections to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "    def tracking_and_segmentation_phase(self, video_path):\n",
    "        print(\"\\n=== PHASE 2: Tracking and Segmentation ===\")\n",
    "        if self.model_manager.sam is None:\n",
    "            print(\"⚠️ SAM not available - skipping\")\n",
    "            return None\n",
    "\n",
    "        if not os.path.exists(self.video_dirs['frames']) or not os.listdir(self.video_dirs['frames']):\n",
    "            print(\"Extracting frames...\")\n",
    "            self.extract_frames(video_path)\n",
    "\n",
    "        frame_files = sorted([f for f in os.listdir(self.video_dirs['frames']) if f.endswith(('.jpg', '.png'))])\n",
    "        mask_metadata = {}\n",
    "        frame_shape = cv2.imread(os.path.join(self.video_dirs['frames'], frame_files[0])).shape[:2]\n",
    "\n",
    "        # Create a new directory for segmented-only frames\n",
    "        segmented_only_dir = os.path.join(self.video_dirs['base'], 'segmented_only', self.current_video, 'frames')\n",
    "        os.makedirs(segmented_only_dir, exist_ok=True)\n",
    "\n",
    "        for obj_id in self.manual_tracks:\n",
    "            self.track_history[obj_id] = {\n",
    "                'positions': deque(maxlen=Config.MIN_TRACK_LENGTH*2),\n",
    "                'frame_count': 0,\n",
    "                'frames': {},\n",
    "                'last_mask': None,\n",
    "                'last_box': self.manual_tracks[obj_id]['first_box'],\n",
    "                'occlusion_count': 0,\n",
    "                'max_occlusion': 5,\n",
    "                'velocity': None\n",
    "            }\n",
    "\n",
    "        start_idx = min(self.manual_tracks[obj_id]['first_frame'] for obj_id in self.manual_tracks)\n",
    "        obj_ids = list(self.manual_tracks.keys())\n",
    "\n",
    "        for frame_idx in tqdm(range(start_idx, len(frame_files)), desc=\"Processing frames\"):\n",
    "            frame_file = frame_files[frame_idx]\n",
    "            frame_path = os.path.join(self.video_dirs['frames'], frame_file)\n",
    "            frame = cv2.imread(frame_path)\n",
    "            if frame is None:\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            self.model_manager.sam.set_image(frame_rgb)\n",
    "\n",
    "            if frame_idx not in mask_metadata:\n",
    "                mask_metadata[frame_idx] = []\n",
    "            tracks = []\n",
    "            masks = []\n",
    "\n",
    "            for obj_id in obj_ids:\n",
    "                history = self.track_history[obj_id]\n",
    "                if history['occlusion_count'] > history['max_occlusion']:\n",
    "                    masks.append(None)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    use_point = False\n",
    "                    prompt = None\n",
    "                    if frame_idx == self.manual_tracks[obj_id]['first_frame']:\n",
    "                        x1, y1, x2, y2 = self.manual_tracks[obj_id]['first_box']\n",
    "                        point = self.manual_tracks[obj_id]['first_point']\n",
    "                        if point is not None:\n",
    "                            prompt = np.array([point])\n",
    "                            use_point = True\n",
    "                        else:\n",
    "                            prompt = np.array([[x1, y1, x2, y2]])\n",
    "                    else:\n",
    "                        last_box = history['last_box']\n",
    "                        if last_box is not None:\n",
    "                            x1, y1, x2, y2 = last_box\n",
    "                            cx = (x1 + x2) / 2\n",
    "                            cy = (y1 + y2) / 2\n",
    "                            if history['velocity'] is not None and history['occlusion_count'] > 0:\n",
    "                                vx, vy = history['velocity']\n",
    "                                cx += vx * history['occlusion_count']\n",
    "                                cy += vy * history['occlusion_count']\n",
    "                                w = x2 - x1\n",
    "                                h = y2 - y1\n",
    "                                x1 = cx - w / 2\n",
    "                                x2 = cx + w / 2\n",
    "                                y1 = cy - h / 2\n",
    "                                y2 = cy + h / 2\n",
    "                                x1 = max(0, min(x1, frame_shape[1] - 1))\n",
    "                                x2 = max(0, min(x2, frame_shape[1] - 1))\n",
    "                                y1 = max(0, min(y1, frame_shape[0] - 1))\n",
    "                                y2 = max(0, min(y2, frame_shape[0] - 1))\n",
    "                            if x1 >= x2 or y1 >= y2:\n",
    "                                history['occlusion_count'] += 1\n",
    "                                masks.append(None)\n",
    "                                continue\n",
    "                            prompt = np.array([[x1, y1, x2, y2]])\n",
    "                        elif history['last_mask'] is not None:\n",
    "                            coords = np.where(history['last_mask'])\n",
    "                            if len(coords[0]) > 0:\n",
    "                                x1, y1 = int(np.min(coords[1])), int(np.min(coords[0]))\n",
    "                                x2, y2 = int(np.max(coords[1])), int(np.max(coords[0]))\n",
    "                                if x1 >= x2 or y1 >= y2:\n",
    "                                    history['occlusion_count'] += 1\n",
    "                                    masks.append(None)\n",
    "                                    continue\n",
    "                                prompt = np.array([[x1, y1, x2, y2]])\n",
    "                                history['last_box'] = [x1, y1, x2, y2]\n",
    "\n",
    "                    if prompt is None:\n",
    "                        history['occlusion_count'] += 1\n",
    "                        masks.append(None)\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        if use_point:\n",
    "                            sam_masks, scores, _ = self.model_manager.sam.predict(\n",
    "                                point_coords=prompt,\n",
    "                                point_labels=np.array([1]),\n",
    "                                multimask_output=True\n",
    "                            )\n",
    "                        else:\n",
    "                            sam_masks, scores, _ = self.model_manager.sam.predict(\n",
    "                                box=prompt,\n",
    "                                multimask_output=True\n",
    "                            )\n",
    "                        best_mask_idx = np.argmax(scores)\n",
    "                        mask = sam_masks[best_mask_idx].astype(np.uint8)\n",
    "                        conf = float(scores[best_mask_idx])\n",
    "                    except Exception:\n",
    "                        history['occlusion_count'] += 1\n",
    "                        masks.append(None)\n",
    "                        continue\n",
    "\n",
    "                    coords = np.where(mask)\n",
    "                    if len(coords[0]) == 0:\n",
    "                        history['occlusion_count'] += 1\n",
    "                        masks.append(None)\n",
    "                        continue\n",
    "\n",
    "                    # Enhance the segmentation\n",
    "                    bbox, mask = self.enhance_segmentation(frame, mask, [x1, y1, x2, y2])\n",
    "                    coords = np.where(mask)\n",
    "                    if len(coords[0]) > 0:\n",
    "                        x1, y1 = int(np.min(coords[1])), int(np.min(coords[0]))\n",
    "                        x2, y2 = int(np.max(coords[1])), int(np.max(coords[0]))\n",
    "                        bbox = [x1, y1, x2, y2]\n",
    "\n",
    "                    history['occlusion_count'] = 0\n",
    "                    cx = (x1 + x2) / 2\n",
    "                    cy = (y1 + y2) / 2\n",
    "\n",
    "                    if len(history['positions']) >= 2:\n",
    "                        prev_cx, prev_cy = history['positions'][-1]\n",
    "                        vx = cx - prev_cx\n",
    "                        vy = cy - prev_cy\n",
    "                        history['velocity'] = (vx, vy)\n",
    "                    history['positions'].append((cx, cy))\n",
    "\n",
    "                    history['frame_count'] += 1\n",
    "                    history['frames'][frame_idx] = {\n",
    "                        'bbox': bbox,\n",
    "                        'conf': conf\n",
    "                    }\n",
    "                    history['last_mask'] = mask\n",
    "                    history['last_box'] = bbox\n",
    "\n",
    "                    mask_path = os.path.join(self.video_dirs['masks'], f\"frame_{frame_idx:04d}_track_{obj_id}.npy\")\n",
    "                    np.save(mask_path, mask)\n",
    "                    mask_metadata[frame_idx].append({\n",
    "                        'track_id': obj_id,\n",
    "                        'bbox': bbox,\n",
    "                        'mask_file': f\"frame_{frame_idx:04d}_track_{obj_id}.npy\",\n",
    "                        'score': conf\n",
    "                    })\n",
    "\n",
    "                    try:\n",
    "                        track = TrackWrapper(obj_id, bbox, conf, last_position=(cx, cy))\n",
    "                        tracks.append(track)\n",
    "                        masks.append(mask)\n",
    "                    except Exception:\n",
    "                        masks.append(None)\n",
    "                        continue\n",
    "\n",
    "                except Exception:\n",
    "                    history['occlusion_count'] += 1\n",
    "                    masks.append(None)\n",
    "                    continue\n",
    "\n",
    "            # Save original frames with boxes and IDs\n",
    "            if tracks:\n",
    "                overlay_frame = self._visualize_results(frame, tracks, masks)\n",
    "                seg_path = os.path.join(self.video_dirs['segmented'], f\"frame_{frame_idx:04d}.jpg\")\n",
    "                cv2.imwrite(seg_path, overlay_frame)\n",
    "                \n",
    "                # Save new frames with only segmented objects\n",
    "                segmented_only_frame = self._visualize_segmented_only(frame, tracks, masks)\n",
    "                seg_only_path = os.path.join(segmented_only_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
    "                cv2.imwrite(seg_only_path, segmented_only_frame)\n",
    "            else:\n",
    "                seg_path = os.path.join(self.video_dirs['segmented'], f\"frame_{frame_idx:04d}.jpg\")\n",
    "                cv2.imwrite(seg_path, frame)\n",
    "                seg_only_path = os.path.join(segmented_only_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
    "                cv2.imwrite(seg_only_path, frame)\n",
    "\n",
    "        valid_tracks = {k: v for k, v in self.track_history.items() if v['frame_count'] >= Config.MIN_TRACK_LENGTH}\n",
    "        self.track_history = valid_tracks\n",
    "\n",
    "        tracking_file = os.path.join(self.video_dirs['tracking_results'], 'tracking_results.json')\n",
    "        with open(tracking_file, 'w') as f:\n",
    "            serializable = {\n",
    "                track_id: {\n",
    "                    'positions': list(history['positions']),\n",
    "                    'frame_count': history['frame_count'],\n",
    "                    'frames': {int(k): v for k, v in history['frames'].items()}\n",
    "                } for track_id, history in self.track_history.items()\n",
    "            }\n",
    "            json.dump(serializable, f)\n",
    "\n",
    "        # Reconstruct original video with boxes\n",
    "        output_path = os.path.join(self.video_dirs['videos'], f\"{self.current_video}_segmented.webm\")\n",
    "        self.reconstruct_video(self.video_dirs['segmented'], output_path, 30)\n",
    "\n",
    "        # Reconstruct new video with only segmented objects\n",
    "        output_segmented_only_path = os.path.join(self.video_dirs['videos'], f\"{self.current_video}_segmented_only.webm\")\n",
    "        self.reconstruct_video(segmented_only_dir, output_segmented_only_path, 30)\n",
    "\n",
    "        metadata_file = os.path.join(self.video_dirs['masks'], 'mask_metadata.json')\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(mask_metadata, f)\n",
    "\n",
    "        print(f\"✅ Segmented video with boxes saved to {output_path}\")\n",
    "        print(f\"✅ Segmented-only video saved to {output_segmented_only_path}\")\n",
    "        print(f\"✅ Tracking results saved to {tracking_file}\")\n",
    "        print(f\"✅ Mask metadata saved to {metadata_file}\")\n",
    "        return output_segmented_only_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN EXECUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear cuda cache to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main function / Custom interface / Run the code\n",
    "-loads labels  \n",
    "-loads model  \n",
    "-iterate through video files  \n",
    "-manual selection / tracking / segmentation  \n",
    "  \n",
    "-add/remove points  \n",
    "-jump to/skip videos  \n",
    "-naviguate through frames  \n",
    "-save output  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(video_dir, output_base_dir):\n",
    "    # Load label files for ground truth\n",
    "    label_files = {\n",
    "        'train': '/home/kassandralea.briola/Project/datasets/something_something_v2/labels/train.json',\n",
    "        'validation': '/home/kassandralea.briola/Project/datasets/something_something_v2/labels/validation.json'\n",
    "    }\n",
    "    \n",
    "    # Map video IDs to ground truth labels and templates\n",
    "    video_label_map = {}  # Maps video_id to {'label': str, 'template': str}\n",
    "    for split in ['train', 'validation']:\n",
    "        try:\n",
    "            with open(label_files[split], 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for item in data:\n",
    "                    video_id = str(item['id'])\n",
    "                    video_label_map[video_id] = {\n",
    "                        'label': item.get('label', 'Unknown'),\n",
    "                        'template': item.get('template', 'Unknown')\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load {split}.json: {str(e)}\")\n",
    "\n",
    "    Config.setup_paths(output_base_dir)\n",
    "    Config.verify_paths()\n",
    "    model_manager = ModelManager(sam_checkpoint=Config.SAM_CHECKPOINT, sam_model_type=Config.SAM_MODEL_TYPE)\n",
    "    if model_manager.sam is None:\n",
    "        print(\"Failed to initialize SAM. Exiting.\")\n",
    "        return\n",
    "\n",
    "    video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.webm', '.avi'))])\n",
    "    current_video_index = 0\n",
    "\n",
    "    while current_video_index < len(video_files):\n",
    "        video_file = video_files[current_video_index]\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        \n",
    "        # Get ground truth label and template\n",
    "        label_info = video_label_map.get(video_name, {'label': 'Unknown', 'template': 'Unknown'})\n",
    "        ground_truth_label = label_info['label']\n",
    "        template = label_info['template']\n",
    "        print(f\"\\nProcessing video: {video_name} (Ground Truth Label: {ground_truth_label}, Template: {template})\")\n",
    "\n",
    "        video_dirs = Config.get_video_dirs(video_name, create=False)\n",
    "        processor = VideoProcessor(video_dirs, model_manager, video_name)\n",
    "\n",
    "        selection_result = processor.manual_selection_phase(video_path)\n",
    "        \n",
    "        # Handle jump action\n",
    "        if isinstance(selection_result, dict) and selection_result.get('action') == 'jump':\n",
    "            video_to_jump = input(\"Enter exact video name to jump to (without extension): \").strip()\n",
    "            matching_videos = [i for i, v in enumerate(video_files) \n",
    "                              if os.path.splitext(v)[0] == video_to_jump]\n",
    "            if matching_videos:\n",
    "                current_video_index = matching_videos[0]\n",
    "                # Print ground truth for the jumped-to video\n",
    "                video_name = os.path.splitext(video_files[current_video_index])[0]\n",
    "                label_info = video_label_map.get(video_name, {'label': 'Unknown', 'template': 'Unknown'})\n",
    "                ground_truth_label = label_info['label']\n",
    "                template = label_info['template']\n",
    "                print(f\"Jumping to video: {video_files[current_video_index]} (Ground Truth Label: {ground_truth_label}, Template: {template})\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Video '{video_to_jump}' not found. Available videos:\")\n",
    "                for v in video_files:\n",
    "                    v_name = os.path.splitext(v)[0]\n",
    "                    v_label_info = video_label_map.get(v_name, {'label': 'Unknown', 'template': 'Unknown'})\n",
    "                    print(f\"  - {v_name} (Label: {v_label_info['label']}, Template: {v_label_info['template']})\")\n",
    "                continue\n",
    "            \n",
    "        # Handle skip action or no selections\n",
    "        if selection_result is None or (isinstance(selection_result, dict) and selection_result.get('action') == 'skip'):\n",
    "            current_video_index += 1\n",
    "            continue\n",
    "            \n",
    "        # Normal processing case\n",
    "        for dir_path in video_dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        output_video = processor.tracking_and_segmentation_phase(video_path)\n",
    "        if output_video:\n",
    "            print(f\"Completed processing {video_name}: {output_video}\")\n",
    "        current_video_index += 1\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    video_dir = \"/home/kassandralea.briola/Project/datasets/something_something_v2/videos\"\n",
    "    output_base_dir = \"/home/kassandralea.briola/Project/output\"\n",
    "    process_videos(video_dir, output_base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
